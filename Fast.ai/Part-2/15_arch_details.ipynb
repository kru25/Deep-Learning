{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapters we learn about the CNN in detailed and also about the resent architectures \n",
    "which we were using in all our computer vision models.In this chapter we would be learning about the \n",
    "state of art architectures we used for tabular,NLP and Image segmentation modules.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Architectures Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are in such condition that we can understand the state of art architectures for computer vision\n",
    ",NLP and tabular modules.We would thus be learning about all the details of the different fastai \n",
    "applications.\n",
    "\n",
    "We will also go through the DataPreprocessing pipeline we had used for Images in Siamese Network model\n",
    "and learn how we can build the pretrained models for new applications..\n",
    "\n",
    "We would start with Computer Vision first.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had built Image Classifiers and Image Segmentation for that we used cnn_learner and unet_learner \n",
    "for the models.We would learn about creating the learner objects we used directly in Rest of the \n",
    "chapters in this book.We would learn about what exactly happens when cnn_learner is called and we pass\n",
    "the arguments.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn_learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass architectures through these functions which make the body of the network.Most of the times we\n",
    "passed ResNet as the architecture about which we learnt in the previous chapter that it uses skip\n",
    "connections and identical mapping.As these are pretrained models so weights are downloaded and loaded\n",
    "in ResNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we learnt about transfer learning where for some layers we use pretrained weights and the later\n",
    "layers are discarded and trained again.So the network has to be cut.We discard the final layer from \n",
    "the network which is responsible for specific classification.Everything after adaptive average pooling\n",
    "layer is sliced off.To search from where the layer has to be cut off,we use a dictionary which shows\n",
    "the contents of the model architecture from its body,head everything.It is called \"__model_meta__\" and\n",
    "and we pass the architecture through it.Let's see what is the output we get.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head of the model is part of the model which is used for a particular task.In a CNN,that is adaptive\n",
    "average pooling layer.\n",
    "Stem is the initial layers and the body includes stem also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cut': -2,\n",
       " 'split': <function fastai.vision.learner._resnet_split(m)>,\n",
       " 'stats': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seeing the model dictionary for resnet-50\n",
    "model_meta[resnet50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"__Cut__\" gives the point after which the layers are cut off.All the layers before this point are kept\n",
    "for transfer learning as they are pretrained weights.It is basically cutting off the head.We thus put\n",
    "a new head in our model.We create it using the create_head function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): AdaptiveConcatPool2d(\n",
       "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "    (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "  )\n",
       "  (1): Flatten(full=False)\n",
       "  (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Dropout(p=0.25, inplace=False)\n",
       "  (4): Linear(in_features=20, out_features=512, bias=False)\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Dropout(p=0.5, inplace=False)\n",
       "  (8): Linear(in_features=512, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_head(20,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function we add how many layers we want to add to our model at the end.We can also choose \n",
    "the number of dropouts and the pooling layers to use.Fastai by default adds average pooling and max \n",
    "pooling together and combines them in AdaptiveConcatPool2d layer.It's not done commonly and generally\n",
    "other libraries add one layer at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai by default adds two linear layers and not one in the CNN head part(the final part of the layer)\n",
    ".It is because transfer learning can then be used for applications in diffrent domains.But if a single\n",
    "layer is added,it cannot be used.Two linear layers allow transfer learning to be quick and easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the unet_learner we used for segmentation previously..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unet_learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image segmentation is also a problem belonging to Computer Vision.It is relatively challenging then\n",
    "Classification because every single pixel is labelled and that's why  predictions are like pixel grid.\n",
    "Some other problems such as increasing resolution of the image,converting black and white image to \n",
    "colored(colorization) are also like Segmentation.In all these problems we have an Image as the input\n",
    "which is converted into other image with same dimension but the pixels are labelled.These are also\n",
    "known as \"__generative vision models__\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of modifying the model architecture is same here also.We first cut off the final layers \n",
    "and create a CNN head.Starting with ResNet the adaptive pooling layer and everything after that is \n",
    "removed and replaced with the head we create.\n",
    "\n",
    "CNN classifies the image.We start with a 224 px image in the starting but at the end of the network\n",
    "after we have cut off the head we have a 7 X 7 grid containing convolutional activations.How do we \n",
    "convert it into an Image of 224 px.\n",
    "\n",
    "This is done through deep learning only meaning we use neural networks to build the segmentation mask.\n",
    "At the end of ResNet body we have a 7 X 7 grid so we need to apply some layer which can increase the \n",
    "grid size.One of the easy ways to do this is we replace every pixel in the 7 X 7 grid with 4 pixels\n",
    "in a 2 X 2 grid.These pixels would have same value and this replacement is known as nearest neighbour\n",
    "interpolation.PyTorch has a custom layer for this and we can use that.So first we create a head \n",
    "containing a stride 1 convolutional layer along with 2 nearest neighbor interpolation layers.The \n",
    "results obtained are not that great though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transposed convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposed convolution is also one of the apporaches used.Instead of the convolutional and the nearest\n",
    "neighbor layer we add a half stride convolutional layer.It is Similar to regular convolution but first\n",
    "a zero padding is added between the pixels.Below figure explains that how transposed convolution is \n",
    "applied:-    \n",
    "<img alt=\"A transposed convolution\" width=\"815\" caption=\"A transposed convolution (courtesy of Vincent Dumoulin and Francesco Visin)\" id=\"transp_conv\" src=\"images/att_00051.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure explains that how zero padding is inserted(the grey grids).This results in an \n",
    "increased input size.Transposed convolution can be applied using the fastai's Conv layer and passing \n",
    "the transpose as True which creates transposed convolution.\n",
    "\n",
    "These approaches do not work very well but.As 7 X 7 grid cannot be used to create a 224 X 224 px image\n",
    "as output.There is not enough information which can be represented.Thus skip connections are used \n",
    "which form the basis of the ResNet but instead of skipping convolution layers,the part from the\n",
    "activations in the ResNet body to the activations in the transposed convolution are skipped.This \n",
    "approach is described in a research paper which describes UNet networks used for biomedical image \n",
    "segmentation.Though initially it was developed for biomedical applications,but it can be used for \n",
    "many generative vision tasks such as colorization,super resolutiona and segmentation.\n",
    "The below figure explains the unet architecture in much detailed..\n",
    "<img alt=\"The U-Net architecture\" width=\"630\" caption=\"The U-Net architecture (courtesy of Olaf Ronneberger, Philipp Fischer, and Thomas Brox)\" id=\"unet\" src=\"images/att_00052.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned before the activations from the ResNet body are skipped till the transposed \n",
    "convolutions on the opposite side.The CNN body is present on the left side.It's a regular CNN and not\n",
    "ResNet and it has a 2 X 2 max pooling layer and not stride 2 convolutions.The transposed convolutional\n",
    "layers are on the opposite side.(\"up-conv\").The skip connections are the ones shown in gray arrows \n",
    "from left to right.Since the shape of the network becomes like this that's why it is called \n",
    "\"U-Net\" architecture.\n",
    "\n",
    "Using this architecture the input is not only the 7 X 7 grid in the previous layer but the higher \n",
    "resolution grid in the ResNet head also.This enables Unet to use all the information in the original\n",
    "image which was not possible earlier.\n",
    "\n",
    "Let's write a custom model architecture for SiaMese network which we had also done earlie.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Siamese Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Siamese network there are pairs of images labelled as True or False as input.It tells if both the\n",
    "images belong to the same class or not.Below is a detailed code starting from downloading the data to\n",
    "labelling it and creating the DataLoaders object for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same code we had gone through in the Chapter-11 while discussing midlevel API.It contains\n",
    "a class SiameseImage which subclasses it to contain two images and a boolean (True or False) if the \n",
    "image belongs to the same class or not.A show method is implemented to display the images concatenated\n",
    "into a single one separated by a black line.An if condition is put that if the images are not tensors.\n",
    "Then they are converted into tensors of same size.A zero matrix is created for the separating black \n",
    "line and this class returns concatenated image tensor and the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the modules from fastai.vision\n",
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.PETS)#downloading and extracting the pets dataset\n",
    "files = get_image_files(path/\"images\")#getting the image files\n",
    "\n",
    "class SiameseImage(fastuple): #SiameseImage declaration\n",
    "    def show(self, ctx=None, **kwargs): \n",
    "        img1,img2,same_breed = self #the three parameters passed\n",
    "        #(image1,image2,the boolean if they are of same breed oe not)\n",
    "        if not isinstance(img1, Tensor): #conditions if images are not tensors\n",
    "            if img2.size != img1.size: #test if the images are not of same size\n",
    "                img2 = img2.resize(img1.size)#resize the image if they are not equal size\n",
    "            t1,t2 = tensor(img1),tensor(img2)#convert the images into tensors\n",
    "            t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1)#change the dimension of the image tensors \n",
    "            #using permute\n",
    "            #else if images are tensors\n",
    "        else: t1,t2 = img1,img2\n",
    "        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)#a zero tensor for the black line.The shape \n",
    "        #is similar to the Image tensors\n",
    "        return show_image(torch.cat([t1,line,t2], dim=2), #concatenate the two images and the line.\n",
    "                          title=same_breed, ctx=ctx)\n",
    "        #display the concatenated images using the show_image\n",
    "    \n",
    "#Label function for naming the images.Using Regex extracts the breed name from the filename\n",
    "def label_func(fname):\n",
    "    return re.match(r'^(.*)_\\d+.jpg$', fname.name).groups()[0]\n",
    "\n",
    "class SiameseTransform(Transform):\n",
    "    def __init__(self, files, label_func, splits):\n",
    "        self.labels = files.map(label_func).unique()\n",
    "        self.lbl2files = {l: L(f for f in files if label_func(f) == l) for l in self.labels}\n",
    "        self.label_func = label_func\n",
    "        self.valid = {f: self._draw(f) for f in files[splits[1]]}\n",
    "        \n",
    "    def encodes(self, f):\n",
    "        f2,t = self.valid.get(f, self._draw(f))\n",
    "        img1,img2 = PILImage.create(f),PILImage.create(f2)\n",
    "        return SiameseImage(img1, img2, t)\n",
    "    \n",
    "    def _draw(self, f):\n",
    "        same = random.random() < 0.5\n",
    "        cls = self.label_func(f)\n",
    "        if not same: cls = random.choice(L(l for l in self.labels if l != cls)) \n",
    "        return random.choice(self.lbl2files[cls]),same\n",
    "    \n",
    "splits = RandomSplitter()(files)#training and validation split\n",
    "tfm = SiameseTransform(files, label_func, splits)#labelling the images\n",
    "tls = TfmdLists(files, tfm, splits=splits)#get the transformed lists of the images\n",
    "dls = tls.dataloaders(after_item=[Resize(224), ToTensor], #creating dataloaders\n",
    "    after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)],num_workers=0,bs=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a custom model and train thid model using a pretrained architecture.It takes two \n",
    "images as input and concatenates the result and passes them through a head part which returns two \n",
    "predictions.We would create head and body of the model ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating custom model for SiameseModel\n",
    "class SiameseModel(Module):\n",
    "    def __init__(self, encoder, head):\n",
    "        self.encoder,self.head = encoder,head#encoder and the head part of the model\n",
    "    \n",
    "    def forward(self, x1, x2):#pass the two images\n",
    "        ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim=1)#pass them through encoder and \n",
    "        #concatenate them\n",
    "        return self.head(ftrs)#pass them through the head part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define encoder now for encoding the image.It is done using the resnet architecture.That is\n",
    "done using the function create_body through which we pass the resnet34 architecture and cut=-2 so as \n",
    "to cut the last two layers which will be our custom layers we would be defining again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating encoder(body) of the model\n",
    "encoder = create_body(resnet34, cut=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder's last layer has 512 features.So the last layers or the head part would have input doubled \n",
    "because we have two images and it would be doubled again because of the concat pooling layer which \n",
    "adds two layers by default.So the head part of the model is created using the create_head function \n",
    "through which we pass the total no of features:512 X 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_head??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating head part\n",
    "head = create_head(512*4, 2, ps=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we pass the encoder and head through the SiameseModel class to get the final model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing encoder,head through the SiameseModel class.\n",
    "model = SiameseModel(encoder, head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create learner.But before that we have to define the loss function.We use CrossEntropyLoss\n",
    "() only but we create a loss_func separately because the targets are in booleans and not integers.So\n",
    "They are first converted into integers and then returned from the loss_func.We pass the targets and \n",
    "predictions through the loss_func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining loss function\n",
    "def loss_func(out, targ):\n",
    "    return nn.CrossEntropyLoss()(out, targ.long())#returns cross entropy loss betweeen the predictions\n",
    "#and the targets converted into integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a splitter function.It tells fastai library to split the model into parameter groups.\n",
    "They are used to train the head of the model.Two different parameter groups are needed for the encoder\n",
    "and the head.\"__params__\" function gives all parameters of the passed architecture.Thus we pass the \n",
    "encoder and head differently through paramas to return the separate parameters for both.In the below \n",
    "code cell the function \"__siamese_splitter__\" is defined which returns these separate parameters for\n",
    "head and encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitter function for splitting the parameters\n",
    "def siamese_splitter(model):\n",
    "    return [params(model.encoder), params(model.head)]\n",
    "#returns the parameter sets for encoder and head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we have defined loss function and splitter we can define the learner by passing the dataloaders,the model architecture,the loss function,the splitter and the metrics used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aren't directly using the cnn_learner so the previous layers prior to head are frozen using learn.\n",
    "freeze().Then we train the last head of the model in the usual way by training for some epochs and a\n",
    "learning rate of 3 X 10^-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the learner \n",
    "learn = Learner(dls, model, loss_func=loss_func, \n",
    "                splitter=siamese_splitter, metrics=accuracy)\n",
    "learn.freeze()#freezing the previous layers of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.540838</td>\n",
       "      <td>0.395717</td>\n",
       "      <td>0.828823</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.424815</td>\n",
       "      <td>0.346322</td>\n",
       "      <td>0.841001</td>\n",
       "      <td>01:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.328580</td>\n",
       "      <td>0.213767</td>\n",
       "      <td>0.921516</td>\n",
       "      <td>01:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.296726</td>\n",
       "      <td>0.183778</td>\n",
       "      <td>0.924222</td>\n",
       "      <td>01:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Training the last layer only for 4 epochs.\n",
    "learn.fit_one_cycle(4, 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We unfreeze the previous layers and train the whole model then with discriminative learning rates that\n",
    "is increasing learning rate with increasing layers.lower for the body but higher for head.We thus\n",
    "train the model by passing 4 epochs and a range of learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.270499</td>\n",
       "      <td>0.176588</td>\n",
       "      <td>0.928958</td>\n",
       "      <td>02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.273814</td>\n",
       "      <td>0.173865</td>\n",
       "      <td>0.927605</td>\n",
       "      <td>02:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.266506</td>\n",
       "      <td>0.177017</td>\n",
       "      <td>0.928281</td>\n",
       "      <td>02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.257422</td>\n",
       "      <td>0.173762</td>\n",
       "      <td>0.927605</td>\n",
       "      <td>02:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#unfreezing the layers\n",
    "learn.unfreeze()\n",
    "#Training the model for 4 epochs and at discriminative learning rates\n",
    "learn.fit_one_cycle(4, slice(1e-6,1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92.7% is a good accuracy for such model.We learnt about creating learners for computer vision models.\n",
    "We can now learn about NLP learners in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had trained an AWD-LSTM language model from scratch in the previous chapters.We can define custom \n",
    "models and turn it into a transfer learning classifier.It can be done in the same way like we did for\n",
    "cnn_learner.We use stacked RNN as the body of the architecture.The encoder provides an activation \n",
    "for every input word.The Language model in general predicts every next word.\n",
    "\n",
    "We use the same approach we had used in the previous chapter for NLP_dive.It uses the one from the \n",
    "ULMFiT paper which is based on the BPTT.(Back Propogation Through time) which means calculating \n",
    "gradients for some past time steps only instead of the whole stream.It detaches the gradient history\n",
    "at every step.\n",
    "\n",
    "We use a for loop in the classifier which iterates over every batch in the sequence.The activation for\n",
    "every batch is stored.At the end they are averaged and max pooled.\n",
    "\n",
    "Though the data is separated into mini-batches but each word is considered as one token as every word\n",
    "has a separate label.The texts are not necessarily of same length though,therefore they cannot be put \n",
    "in the same array.\n",
    "\n",
    "Like in CNN we use padding for images to add some extra pixels we can do the same with text to add\n",
    "some extra words.From the text we identify the text with greatest length and then fill the text with\n",
    "tokens \"xxpad\".In this way texts are made of same size.If there is a text such that it has batches of \n",
    "2000 tokens as well as 10 tokens then a lot of padding tokens have to be added which leads to wastage \n",
    "of memory.So,texts of comparable sizes are put together.Fastai does this automatically while creating\n",
    "dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we would discuss about fastai tabular models.We won't be talking about collaborative filtering\n",
    "models separately as they are similar to tabular models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be talking much about the whole architecture of the TabularModel .We would be talking more\n",
    "about the forward function.\n",
    "Let's see what all is included in the forward method for TabularModel:-\n",
    "The code looks like:-\n",
    "    \n",
    "    ```python\n",
    "if self.n_emb != 0:\n",
    "    x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "    x = torch.cat(x, 1)\n",
    "    x = self.emb_drop(x)\n",
    "if self.n_cont != 0:\n",
    "    x_cont = self.bn_cont(x_cont)\n",
    "    x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "return self.layers(x)\n",
    "```\n",
    "\n",
    "Let us now understand every line of the above code.\n",
    "\n",
    "\"__self.n_emb!=0__\" tests if there are any embeddings in the data.If there are embedding matrices then\n",
    "we can get the activations through \"__x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]__\".They \n",
    "are later combined into a tensor through \"__x = torch.cat(x, 1)__\".Next we apply dropout using the \n",
    "\"emb_drop\".\"__x = self.emb_drop(x)__\".\n",
    "Next we check if there are any continuous variables in the data using \"__if self.n_cont != 0:__\".They\n",
    "are passed through the batch_norm layer using \"__ x_cont = self.bn_cont(x_cont)__\".They are combined\n",
    "with the embedded activations.\"__x = torch.cat([x, x_cont], 1)__\".At the end it is passed through \n",
    "three linear layers.And the activations are returned.\n",
    "\n",
    "\n",
    "We have so far discussed about the basics of all the architectures for Image segmentation,\n",
    "classification,NLP and tabular modules in the fastai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we want to understand the beneath details of any architecture,the source code in fastai is of\n",
    "help.It is important to understand the meaning of the code.\n",
    "\n",
    "We have almost investigated about all the steps of building a practical deep learning model now.In \n",
    "case of unlimited data and time we can train a big model for analysis which would then take a large \n",
    "time also to run.However most of the times we have memory and time constraints.That time its better to\n",
    "train a small model.Initially we should train our model till the point that its starts getting overfit\n",
    "and then we should apply some techniques to reduce that.\n",
    "\n",
    "Initial steps include train your model till it gets overfit.Now to reduce overfitting below figure \n",
    "explains a broader view of the steps to be followed:-\n",
    "    \n",
    "<img alt=\"Steps to reducing overfitting\" width=\"400\" caption=\"Steps to reducing overfitting\" id=\"reduce_overfit\" src=\"images/att_00047.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the people prefer using small models or regularization.But that is wrong.Infact using smaller \n",
    "model would be the least preferred step among all.If at all after trying all methods we are not able\n",
    "to reduce overfitting then we can try small models.\n",
    "\n",
    "The first step after overfitting should be to crate more data.It may include adding more labels or \n",
    "adding synthetic data by trying different data augmentation transforms.Mixup is one of the approaches\n",
    "used though there are many available for all kinds of data.\n",
    "\n",
    "After creating enough data,we should follow proper augmentation process.If that also is not reducing\n",
    "overfitting,batch normalization layer can be added.\n",
    "\n",
    "After following these steps if the model still overfits then we can apply regularization.Adding a\n",
    "dropout to the last layer regularizes the model.Adding dropouts of different types helps in making the\n",
    "model more flexible.Thus a large model with more regularization is more flexible and accurate than a \n",
    "small model with less regularization.\n",
    "\n",
    "If any of these options don't work then only we try small architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the \"head\" of a neural net?\n",
    "1. What is the \"body\" of a neural net?\n",
    "1. What is \"cutting\" a neural net? Why do we need to do this for transfer learning?\n",
    "1. What is `model_meta`? Try printing it to see what's inside.\n",
    "1. Read the source code for `create_head` and make sure you understand what each line does.\n",
    "1. Look at the output of `create_head` and make sure you understand why each layer is there, and how the `create_head` source created it.\n",
    "1. Figure out how to change the dropout, layer size, and number of layers created by `cnn_learner`, and see if you can find values that result in better accuracy from the pet recognizer.\n",
    "1. What does `AdaptiveConcatPool2d` do?\n",
    "1. What is \"nearest neighbor interpolation\"? How can it be used to upsample convolutional activations?\n",
    "1. What is a \"transposed convolution\"? What is another name for it?\n",
    "1. Create a conv layer with `transpose=True` and apply it to an image. Check the output shape.\n",
    "1. Draw the U-Net architecture.\n",
    "1. What is \"BPTT for Text Classification\" (BPT3C)?\n",
    "1. How do we handle different length sequences in BPT3C?\n",
    "1. Try to run each line of `TabularModel.forward` separately, one line per cell, in a notebook, and look at the input and output shapes at each step.\n",
    "1. How is `self.layers` defined in `TabularModel`?\n",
    "1. What are the five steps for preventing over-fitting?\n",
    "1. Why don't we reduce architecture complexity before trying other approaches to preventing overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write your own custom head and try training the pet recognizer with it. See if you can get a better result than fastai's default.\n",
    "1. Try switching between `AdaptiveConcatPool2d` and `AdaptiveAvgPool2d` in a CNN head and see what difference it makes.\n",
    "1. Write your own custom splitter to create a separate parameter group for every ResNet block, and a separate group for the stem. Try training with it, and see if it improves the pet recognizer.\n",
    "1. Read the online chapter about generative image models, and create your own colorizer, super-resolution model, or style transfer model.\n",
    "1. Create a custom head using nearest neighbor interpolation and use it to do segmentation on CamVid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-1 Head of a neural net is that part of the network which is responsible for doing a specific task\n",
    "in the network.Mostly these are the last layers of the network always.\n",
    "\n",
    "Ans-2 Body of the network refers to everything in the model except the head part.It includes initial \n",
    "layers and the encoding part of the model.Body ends where the head starts.\n",
    "\n",
    "Ans-3 Cutting a neural network means slicing off a part of neural network.Generally the final layers\n",
    "are discarded off.In transfer learning it is important because rest part of the model contains \n",
    "pretrained layers.They don't need more training whereas the head of the model does specific task for\n",
    "specific problem,so it has to be trained all over again.That's why the pretrained weights in the last\n",
    "layers are discarded.\n",
    "\n",
    "Ans-4 model_meta gives the dictionary of information we require to create a custom model from \n",
    "pretrained architecture.We just need to pass the architecture name through it.\n",
    "\n",
    "Ans-8 AdaptiveconcatPool2d is a layer provided by fastai which by default applies average pooling and\n",
    "max pooling together and concatenates its results.\n",
    "\n",
    "Ans-9 If we have an activation grid of size 7 X 7 then we replace each pixel by 4 pixels in a grid \n",
    "size of 2 X 2.The values of all the four pixels in the square would be same and this is known as \n",
    "nearest neighbor interpolation.This allows us to increase the grid size in the CNN.\n",
    "\n",
    "Ans-10 To upsample the grid size in CNN,we use transposed convolution.We do a regular convolution here\n",
    "only difference is that we first insert a zero padding.To get a transposed convolution,we have to pass\n",
    "the parameters Transpose=True in the fastai's ConvLayer in the head we would create for the model.\n",
    "Also known as stride-half convolution.\n",
    "\n",
    "Ans-13 BPTT(BackPropogation through time) is used for updating the weights when RNN is used for some\n",
    "sequential data like time series.The gradients are updated at each time step to save memory and time.\n",
    "\n",
    "Ans-14 To handle different length texts in batches,we do padding by filling the texts which have short\n",
    "length with special tokens called \"xxpad\".\n",
    "\n",
    "Ans-16 In \"__init__\".\n",
    "\n",
    "Ans-17 Collect more data,Data Augmentation,Adding generalized layers,Regularization,reduce model\n",
    "complexity by trying small models.\n",
    "\n",
    "Ans-18 Reducing model complexity in initial steps would reduce the ability of the model to learn \n",
    "important relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
