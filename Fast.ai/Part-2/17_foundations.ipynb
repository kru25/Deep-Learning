{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last previous chapters we have learnt about almost all aspects of training a model from\n",
    "callbacks,different optimizers,creating custom models etc.In this chapter we will be focussing on\n",
    "training things from scratch.Most of the things we will be doing will be the ones which we have \n",
    "already studied.But this time it will be more focussed on implementation and not on practical \n",
    "meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Net from the Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be starting with basic tensor indexing,then we will come to neural nets,then implementing\n",
    "backpropogation and how loss is calculated through PyTorch.We would also learn about autograd\n",
    "package in PyTorch which directly calculates gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Net Layer from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we would be building neural nets from scratch and implement loss calculation and backward\n",
    "and forward propogation,we can start with revising some Linear Algebra basics.Initially we would \n",
    "use python to implement everything and then later we would replace everything with PyTorch \n",
    "functionality and see how many lines of code can be replaced by single one.We would feel that \n",
    "we are doing the same things again but this chaptwe would focus more on implementation.Let's \n",
    "start with some theory for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling a Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neuron gets a number of inputs and has a corresponding set of weights for each one.The weights \n",
    "are multiplied by inputs and then a bias is added to them.Mathematically it can be represented as\n",
    ":\n",
    "    $$ out = \\sum_{i=1}^{n} x_{i} w_{i} + b$$\n",
    "\n",
    "Here xi(1.....n) are the inputs and wi(1....n) are the weights and b is the bias.While in code \n",
    "this can be written as :-\n",
    "    \n",
    "output = sum([x*w for x,w in zip(inputs,weights)]) + bias\n",
    "\n",
    "\n",
    "A nonlinear function known as \"Activation function\" is then applied to this output after which \n",
    "the information is passed to another neuron.The function is mostly Rectified linear unit mostly\n",
    "called \"ReLU\" which basically zeros every quantity less than zero.The function can be returned as\n",
    "\n",
    "def relu(x): \n",
    "    return x if x >= 0 else 0\n",
    "\n",
    "A deep learning model consists of many such neurons stacked in layers.Generally a first layer is\n",
    "created with neurons equal to the input size and the inputs are connected to these neurons.This \n",
    "layer is also called fully connected layer or a linear layer.Next we compute the dot product of \n",
    "each input and each weight and sum them up.\n",
    "\n",
    "sum([x*w for x,w in zip(input,weight)])\n",
    "\n",
    "They are added because its a matrix multiplication.Let's say if input x has a size of batch_size\n",
    "X n_inputs and weights are such that in matrix w,there are n_neurons X n_inputs.Every neuron has \n",
    "same number of weights as there are inputs.And the bias numbers are in a vector(1 D tensor) with\n",
    "size n_neurons.(Every neuron has one tensor).Thus the output would be:-\n",
    "    \n",
    "    y=x @ w.t()+b\n",
    "    \n",
    "Here @ is the matrix multiplication product and.t() takes the transpose of w matrix.y is then of \n",
    "the size batch_size X n_neurons.Mathematically written as:\n",
    "    \n",
    "$$y_{i,j} = \\sum_{k=1}^{n} x_{i,k} w_{k,j} + b_{j}$$\n",
    "\n",
    "The transpose has to be taken because mathematically in  m @ n , the coefficient at any position\n",
    "(i,j) is given by:-\n",
    "\n",
    "sum([a * b for a,b in zip(m[i,:],n[:,j])])\n",
    "\n",
    "So like for every calculation we need matrix multiplication.Let's learn about matrix \n",
    "multiplication then...    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would write a function which would do multiplication of two tensors and return the product \n",
    "using plain Python.Though we can do it directly  using PyTorch function but to understand how it\n",
    "is done we would use Python.We would be using only the PyTorch indexing here instead of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing PyTorch\n",
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function matmul and pass 2 matrices a and b through it which are to be multiplied.\n",
    "Next we store a's rows and columns in ar,ac respectively and b's in br and bc.Then we make sure\n",
    "that a's columns are equal to b's rows.Next we create a zero matrix for our product of size a's \n",
    "rows X b's columns.Then we use nested loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function matmul we use three nested loops one for row indices,one for column indices\n",
    "and one for the sum.ac and ar are number of columns and number of rows of a and the same for br,\n",
    "bc and before multiplying we check that a has as many as columns as number of rows in b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the loops we do multiplication.c is the product matrix containing zeros.But we multiply \n",
    "the corresponding elements of a and b and then add them for every column.Next we return the \n",
    "product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for matrix multiplication\n",
    "def matmul(a,b):\n",
    "    ar,ac = a.shape # n_rows * n_cols\n",
    "    br,bc = b.shape\n",
    "    assert ac==br#check if the a's columns are equal to b's rows\n",
    "    c = torch.zeros(ar, bc)#initialize a zero matrix for product\n",
    "    for i in range(ar):#looping through a's rows\n",
    "        for j in range(bc):#b's columns\n",
    "            for k in range(ac): #a's columns\n",
    "                c[i,j] += a[i,k] * b[k,j]#matrix multiplication\n",
    "    return c #return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to implement this function we create two random matrices.The sizes would be such that it is \n",
    "equal to that of MNIST images.We would be using 5 MNIST images of size 28 X 28.And a linear model\n",
    "so that we get 10 activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating random matrices for matrix multiplication\n",
    "m1 = torch.randn(5,28*28)#5 MNIST images of size 28 X 28 pixels\n",
    "m2 = torch.randn(784,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we pass both the matrices through matmul to multiply them.We would be using Jupyter's magic \n",
    "command called %time which would return the time taken for the whole computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%time t1=matmul(m1, m2) #time taken for multiplication through function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use PyTorch's built in method of @ and print the time taken for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 12.52 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "194 µs ± 291 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 20 t2=m1@m2 #multiplication using @ method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we multiply matrices using the user define function it takes time in milli secs but when it \n",
    "is done through @ , the time is a few microseconds.So using Python's three nested loops to\n",
    "multiply is not good as it takes more time whereas PyTorch's direct @ implementation is 10000\n",
    "times faster than Python and if done on GPU,it's more faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because PyTorch is written in C++ so as to make it fast.It's easy on PyTorch because we\n",
    "use tensors for calculation and also Broadcasting and Elementwise operations makes it more fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us learn more about the element wise operations on matrices in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply all basic mathematical operations such as +,-,=,*,/,%,==,<,> elementwise in PyTorch.\n",
    "So if we do a+b then all the elements in a and b will be added elementwise so they should be of\n",
    "same shape and we get a tensor of same shape having sum of elements of a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12., 14.,  3.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#addition of two tensors\n",
    "a = tensor([10., 6, -4])\n",
    "b = tensor([2., 8, 7])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<,> are Boolean operators.They return True or False so we get a tensor with True or False as \n",
    "elements when we compare two matrices of equal shape using >/<."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comparing elements using Boolean operators\n",
    "a < b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some operators which return  tensors with only one element after applying them on a \n",
    "tensor with more than one elements.Some of these are all(),sum() and mean().To get the one \n",
    "element from the 0 ranked tensor we use .item()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(False), tensor(False))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reduction operations on tensors\n",
    "(a < b).all(), (a==b).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.666666984558105"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting single element from reduction operation using .item() method.\n",
    "(a + b).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elementwise operations can be applied on tensors of any rank provided they have same shape.\n",
    "Let's see what happens when they are of different shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.,  9.],\n",
       "        [16., 25., 36.],\n",
       "        [49., 64., 81.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiplying the tensors of same rank and shape\n",
    "m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n",
    "m*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape,n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e3b5f7b55acc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Defining other tensor of different shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn\u001b[0m \u001b[1;31m#doesn't work as the shapes do not much\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "#Defining other tensor of different shape\n",
    "n = tensor([[1., 2, 3], [4,5,6]])\n",
    "m*n #doesn't work as the shapes do not much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m X n is not possible as the shapes of the tensors do not match.We know that using PyTorch \n",
    "functionality speeds things up so we multiply the tensors such that ith row of a and jth column \n",
    "of b are multiplied and then all the elements are summed up.This will be executed fast as the \n",
    "third inner loop is executed by PyTorch at a faster speed.\n",
    "\n",
    "To access one column or one row for a tensor we use a[:,j] or a[i,:].The : means all elements.\n",
    "We can also use range using i:j.It would then take elements from i to j.j is noninclusive here \n",
    "though.Instead of a[i,:] we can also use a[i].Let's implement the reduction function then and \n",
    "remove the third loop from the function and see how much less time it takes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function matmul after removing the inner for loop\n",
    "def matmul(a,b):#pass two matrices\n",
    "    ar,ac = a.shape#row and columns of a \n",
    "    br,bc = b.shape#row and columns of b\n",
    "    assert ac==br#test if a's columns are equal to b's rows\n",
    "    c = torch.zeros(ar, bc)#initalize product matrix c\n",
    "    for i in range(ar):#iterate through a's rows\n",
    "        for j in range(bc):#iterate through b's columns\n",
    "            c[i,j] = (a[i] * b[:,j]).sum()#multiplying and summing up using.sum()\n",
    "    return c #return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use magic command %timeit to check the time taken for executing this function.So we pass\n",
    "m1 and m2 through the function and we print the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.85 ms ± 128 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 20 t3 = matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is around 1000 times faster already then the function with one extra for loop.Using \n",
    "broadcasting we can remove all loops and speed up the function more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting is a term we came across earlier also.It tells how operations are performed between \n",
    "the tensors of different ranks.If there is a 3 X 3 matrix and a 4 X 5 matrix obviously we cannot\n",
    "add them but if we add scalar to a matrix we can do it using broadcasting.A vector of size 3 can\n",
    "be added to a tensor of size 3 X 4.Let's see how do we do it.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting basically gives some rules to test about the compatiblity of tests when we are doing\n",
    "a mathematical operation.The tensor with smaller shape is basically expanded to match with the \n",
    "tensor of bigger shape.Let's understand these rules first using some examples.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting with a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is the simplest broadcasting.When out of the two matrices between which we want to perform \n",
    "operations,one of them is a scalar and one of them is a tensor,then a tensor of same shape as\n",
    "that of the given tensor is created consisting of the scalar as the elements and then the \n",
    "operation is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True, False])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comparing tensor with a scalar\n",
    "a = tensor([10., 6, -4])\n",
    "a > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 0 is broadcasted to a shape and then compared with each element in a.Thus you get the same\n",
    "shape tensor in output.This is also very useful if we are normalizing dataset by subtracting mean\n",
    "and dividing it by standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4652, -1.0989, -0.7326],\n",
       "        [-0.3663,  0.0000,  0.3663],\n",
       "        [ 0.7326,  1.0989,  1.4652]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalizing a tensor using broadcasting\n",
    "m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])\n",
    "(m - 5) / 2.73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case if the means for rows are different,we broadcast a vector to matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting a vector to a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3]), torch.Size([3]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#broadcasting vector to a matrix\n",
    "c = tensor([10.,20,30])#vector\n",
    "m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])#matrix\n",
    "m.shape,c.shape#their shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 22., 33.],\n",
       "        [14., 25., 36.],\n",
       "        [17., 28., 39.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding them using broadcasting\n",
    "m + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c has a shape of 1 X 3 and m has shape 3 X 3.So,c is expanded in such a way that it has three \n",
    "rows and three columns.Actual copy is not created but addition happens in such a way.Using \n",
    "expand_as method the expansion happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 20., 30.],\n",
       "        [10., 20., 30.],\n",
       "        [10., 20., 30.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Expanding vector using expand_as\n",
    "c.expand_as(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storage property can be checked for any tensor using .storage() method to check for useless data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 10.0\n",
       " 20.0\n",
       " 30.0\n",
       "[torch.FloatStorage of size 3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#expanding the tensor\n",
    "t = c.expand_as(m)\n",
    "t.storage()#getting storage using the storage method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though t is of size 3 X 3,it takes the space for only 3 float elements.It is because the dim has \n",
    "a stride of zero.We can check the stride using .stride() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 1), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stride and shape of expanded tensor\n",
    "t.stride(), t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the broadcasting was done on last dimension.If it's done otherwise still the result is\n",
    "same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 22., 33.],\n",
       "        [14., 25., 36.],\n",
       "        [17., 28., 39.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Broadcasting other side\n",
    "c + m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a vector is of size n then it can be broadcasted with a matrix of size m X n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 22., 33.],\n",
       "        [14., 25., 36.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Broadcasting \n",
    "c = tensor([10.,20,30])#vector\n",
    "m = tensor([[1., 2, 3], [4,5,6]])#multidimensional tensor\n",
    "c+m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above c has a size of 3 and m has a size of 2 X 3.So broadcasting is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-b427e8ac055b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#c shape is 2,m shape is 2 X 3,so not compatible for broadcasting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#Broadcasting not possible\n",
    "c = tensor([10.,20])\n",
    "m = tensor([[1., 2, 3], [4,5,6]])\n",
    "c+m\n",
    "#c shape is 2,m shape is 2 X 3,so not compatible for broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To broadcast in other dimension,the vector should be of shape 3 X 1.To add a unit dimension,we \n",
    "use unsqueeze method in PyTorch.We had done this earlier also while implementing cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding unit dimension to a vector \n",
    "c = tensor([10.,20,30])# vector\n",
    "m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])#tensor\n",
    "c = c.unsqueeze(1)#unsqueeze method\n",
    "m.shape,c.shape#shape of the tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector has a shape of 3 X 1 now.And it is expanded on the column side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11., 12., 13.],\n",
       "        [24., 25., 26.],\n",
       "        [37., 38., 39.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#applying broadcasting during addition\n",
    "c+m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like previously even this expanded tensor though contains 9 elements but has a storage of 3 \n",
    "elements only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 10.0\n",
       " 20.0\n",
       " 30.0\n",
       "[torch.FloatStorage of size 3]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the storage of the expanded tensor\n",
    "t = c.expand_as(m)\n",
    "t.storage()#3 float elements stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the stride and shape of the expanded tensor.Even this tensor has a 0 stride in column\n",
    "dimension.It can be checked using .stride() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 0), torch.Size([3, 3]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the stride and shape of the expanded tensor\n",
    "t.stride(), t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we expand tensors using unsqueeze method it adds a unit dimension at the beginning by\n",
    "default.But we can pass the index where the dimension has to be added.Let's see how it works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How unsqueeze method adds unit dimension at the required position\n",
    "c = tensor([10.,20,30])\n",
    "c.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unsqueeze method can also be replaced using None indexing.None indexing is like wherever we\n",
    "want to add the dimension we can add None as the position while indexing the particular dimension\n",
    ".For eg a[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding None indexing to add dimension at the beginning or end.\n",
    "c.shape, c[None,:].shape,c[:,None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While indexing the trailing colons can be omitted and instead we can just use None indexing or \n",
    "... which stands for previous dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([3, 1]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Omitting the trailing colons\n",
    "c[None].shape,c[...,None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to make the multiplication faster we can remove one more for loop and when we multiply a[i]\n",
    "with b[:,j] instead of that we multiply a[i] with whole b via broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing one more for loop using the unsqueeze method\n",
    "def matmul(a,b):# a and b are passed\n",
    "    ar,ac = a.shape#a's rows and columns\n",
    "    br,bc = b.shape#b's rows and columns\n",
    "    assert ac==br#check if the a's columns and b's rows are equal\n",
    "    c = torch.zeros(ar, bc)#initialze the final product to zero matrix.\n",
    "    for i in range(ar):\n",
    "#       c[i,j] = (a[i,:]          * b[:,j]).sum() # previous\n",
    "        c[i]   = (a[i  ].unsqueeze(-1) * b).sum(dim=0)\n",
    "    return c#returning the product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we again see the time taken for executing matmul after removing another for loop and how \n",
    "fast it has become.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496 µs ± 35.3 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n"
     ]
    }
   ],
   "source": [
    "#time taken for multiplication\n",
    "%timeit -n 20 t4 = matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing one more for loop,it has become a lot more effecient in terms of time.Let's learn\n",
    "more about the Broadcasting rules..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever any operation is performed between two tensors,PyTorch compares elements of both the \n",
    "tensors.Two dimensions of two tensors are said to be compatible only when:-\n",
    "They are equal or if one of them is 1,then it  can be broadcasted to match the shape of \n",
    "the other one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having same number of dimensions is not compulsory.If there is an image of size 256 X 256 X 3\n",
    "array containing RGB values.To scale the image we can multiply it by a vector with 3 values.\n",
    "Broadcasting causes it to give a result of 3 d array only.However if the vector has 2 elements,\n",
    "then it's not compatible.If the image is a 3 X 3 matrix and vector is a 1d tensor then also \n",
    "broadcasting happens over rows.\n",
    "\n",
    "Let's learn about Einstein summation now..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einstein Summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from PyTorch's @ and torch.matmul there is one more method in which we can implement matrix\n",
    "multiplication called Einstein summation or einsum.It can be written as :-\n",
    "    \n",
    "    ik,kj -> ij\n",
    "\n",
    "Here we multiply two tensors of size(i X k) and (k X j) and the product is of size (i X j).We\n",
    "have also studied this in High School Mathematics.In PyTorch it's available as torch.einsum.\n",
    "The rules for einstein summation are as follows:-\n",
    "\n",
    "1.Repeated indices are implicitly summed.\n",
    "2.Every index can appear at most twice in any term.\n",
    "\n",
    "Here k is repeated so summed over k.The product matrix contains elements at positions (i,j)\n",
    "such that they are the sum of coefficients in (i,k) multiplied by (k,j) in the other tensor and \n",
    "we get the matrix product.In PyTorch it is implemented as follows:-\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing PyTorch's Einstein summation\n",
    "def matmul(a,b): \n",
    "    return torch.einsum('ik,kj->ij', a, b)# a and b are the matrices which are to be multiplied\n",
    "# and before that the dimensions of the operands and products are passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much time it takes for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 6.04 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "180 µs ± 168 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n"
     ]
    }
   ],
   "source": [
    "#Time taken for einstein summation\n",
    "%timeit -n 20 t5 = matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a lot faster than all the previous functions we had executed for matrix multiplication.\n",
    "\"einsum\" is thus one of the fastest ways for operations in PyTorch.We have learnt enough about \n",
    "matrix multiplication.We can build the neural net now..So let's implement the forward and \n",
    "backward pass now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Forward and Backward Passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass and backward pass together make an epoch.Forward pass is the calculation process \n",
    "where the output is calculated from the input values while traversing through the layers.A loss \n",
    "function is calculated from output and input values.During the backward pass using gradient \n",
    "descent algorithm the weights are updated from last layer till first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus during the forward pass we calculate the outputs based on matrix products and gradients in \n",
    "backward pass.Further we will define the neural net and will also learn about initializing the \n",
    "weights properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and Initializing a Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter while working with MNIST sample image model we had tried a simple two \n",
    "layer model.Here also we would be starting with that.The first layer is a simple linear layer \n",
    "and does the calculation wx+b where w and x are inputs and weights and b is the bias.So we simply\n",
    "define a function lin which takes input(x),weights(w) and bias(b) as arguments and returns \n",
    "x @ w + b where @ represents matrix multiplication as w,x are multidimensional tensors and b \n",
    "also is a 1d vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining first linear layer of the neural net\n",
    "def lin(x, w, b): #inputs,weights,bias\n",
    "    return x @ w + b #return wx+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we had seen that we pass the output through a non-linear function called \"activation \n",
    "function\" to add non-linearity to the model.One of the common activation functions used in deep\n",
    "learning for inner layers is ReLU returning maximum of 0 and x.Before passing the inputs through\n",
    "the model let's create a random set of inputs and outputs.Since we won't train our model,so \n",
    "random inputs and outputs are created.\n",
    "Thus we create tensors of size 200 X 100 as inputs and a vector of 200 values as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating random inputs and output\n",
    "x = torch.randn(200, 100)#input 200 X 100\n",
    "y = torch.randn(200)#output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are creating a two layer model so we would have 2 sets of parameters(weights and bias).\n",
    "So we would have 2 weight matrices and 2 bias vectors.We initalize the parameters randomly by\n",
    "creating first weight matrix of size 100 X 50 as hidden size is 50 and the output size is 1.The\n",
    "output is one float.Weight matrices are initialized randomly and bias is initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing weight matrices and bias vectors for two layers\n",
    "w1 = torch.randn(100,50)#weight matrix for layer 1(100 X hidden size)\n",
    "b1 = torch.zeros(50)#bias vector for layer 1\n",
    "w2 = torch.randn(50,1)#weight matrix for layer 2\n",
    "b2 = torch.zeros(1)#bias vector for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass the inputs,weight matrix and bias through the first linear layer by calling the \n",
    "function lin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 50])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#passing input,weight matrix and bias through the first linear layer \n",
    "l1 = lin(x, w1, b1)#input,weight matrix,bias\n",
    "l1.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer returns an output of size 200,the batch size by 50,the hidden size.Let's see what\n",
    "is the loophole in the way we initialized the parameters and input to the model.For that we would\n",
    "look at the mean and standard deviation of the output from first layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0284), tensor(10.0478))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean and standard deviation of the output from first layer\n",
    "l1.mean(), l1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean is fine as it is near to zero also since the inputs and weights also have means close to\n",
    "zero.The standard deviation basically represents how much the values are deviated from mean.The\n",
    "number can go upto 10 that too in just one layer.This is a problem.Most of the networks have 100s\n",
    ",1000s of layers and if the activations are multiplied by a scale of 10 everytime we know that \n",
    "there is a scale upto which numbers are stored in computers.If it goes beyond that then it's a \n",
    "problem.\n",
    "\n",
    "Let's just randomly multiply the input with 100 X 100 random matrix 50 times and see if we can \n",
    "get the representable pixel values.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Randomly multiplying input by 100 x 100 matrix to reduce the standard deviation\n",
    "x = torch.randn(200, 100)#initalizing input\n",
    "for i in range(50):#iterating 50 times\n",
    "    x = x @ torch.randn(100,100)#multiply by 100 X 100 random matrix\n",
    "x[0:5,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting matrix is full of nans.So we can use more smaller parameters but then it would \n",
    "keep getting smaller and after continued multiplication in 100 layers,the values obtained are \n",
    "almost zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using a small scale then and see what we have in output.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using smaller scale of parameters \n",
    "x = torch.randn(200, 100)\n",
    "for i in range(50): #iterating 50 times assuming 50 layers\n",
    "    x = x @ (torch.randn(100,100) * 0.01)#multiplying by 0.01 for lower scale \n",
    "x[0:5,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that we got zeros after using lower scale parameters.So weight matrices should be \n",
    "scaled such that activations have a standard deviation near to 1.A research work by Xavier Glorot\n",
    "and Yoshua Bengio has published that the scale for a layer is given by 1/sqrt(Nin) where Nin is \n",
    "the number of inputs to the layer.\n",
    "\n",
    "Here the number of inputs to the layer are 100 so,the scale would be 0.1 So let's try scaling by\n",
    "0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3485e+00,  1.1395e+00,  1.5915e-03, -6.0329e-02, -5.0502e-01],\n",
       "        [ 2.7586e+00, -1.9528e-01,  1.1850e-02, -1.3599e+00,  8.3320e-02],\n",
       "        [ 3.2456e+00,  5.3009e-01, -1.3298e-01, -7.4117e-01, -6.1467e-01],\n",
       "        [-1.4938e+00, -2.0137e-02,  2.0974e-02,  1.9033e-01,  4.9550e-01],\n",
       "        [ 3.5250e-01,  1.2939e-01, -7.8426e-02,  1.8014e-01, -2.8594e-01]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling the weight matrix  by 0.1\n",
    "x = torch.randn(200, 100)#initializing the input matrix\n",
    "for i in range(50): #iterating over 50 layers\n",
    "    x = x @ (torch.randn(100,100) * 0.1)#scaling the weight matrix by 0.1\n",
    "x[0:5,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after scaling as per the scaling rule we don't have any zeros or nans in the output.Let's see\n",
    "what is the standard deviation of these activations.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8335)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#standard deviation of the activations\n",
    "x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally we have activations in a stable range with standard deviation within 1.We saw that \n",
    "even varying the scale by 0.1 can give us very small or very large activations so proper \n",
    "initialization of weight matrix is very important.Let's start by defining our inputs again and \n",
    "proceed further with neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the inputs and outputs\n",
    "x = torch.randn(200, 100)\n",
    "y = torch.randn(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we also initalize the weight matrix and bias vectors for both the layers.We also scale the\n",
    "weight matrix according to the right scaling rule.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing weight matrix and bias vectors for both layers\n",
    "from math import sqrt\n",
    "w1 = torch.randn(100,50) / sqrt(100)#scaling by dividing by 0.1\n",
    "b1 = torch.zeros(50)\n",
    "w2 = torch.randn(50,1) / sqrt(50)#scaling by dividing by sqrt(50)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass it through the first layer then.We also see the mean and standard deviation of the \n",
    "activations from the layer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0085), tensor(1.0155))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#passing the input,weight and bias through the first layer..\n",
    "l1 = lin(x, w1, b1)#input(x),weight matrix(w1),bias(b1)\n",
    "l1.mean(),l1.std()#mean and standard deviation of the activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the mean of the activations nearly zero and the standard deviation also close to 1.\n",
    "So that's perfect.Now previously also we had mentioned about passing the linear activations \n",
    "through non-linear activation function to get the final activations.We use ReLU here the most\n",
    "common activation function in deep learning and it replaces the negatives with a zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for relu activation\n",
    "def relu(x): \n",
    "    return x.clamp_min(0.)#clamp_min zeros all negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we pass the linear activations through relu function to get ReLU activation.Let's also check\n",
    "the mean and standard deviation of these activations.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4074), tensor(0.5965))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Passing linear activations through relu\n",
    "l2 = relu(l1)\n",
    "l2.mean(),l2.std()#mean and standard deviation of non-linear activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the activations has again increased to 0.4 and standard deviation is 0.58.Mean\n",
    "increased because reLU removed negative values.Let's check if after few layers this also results\n",
    "in a zero or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5217e-09, 1.5451e-08, 2.6013e-08, 2.1594e-08, 1.0393e-08],\n",
       "        [4.4534e-09, 2.3102e-08, 3.9754e-08, 3.3959e-08, 1.6925e-08],\n",
       "        [0.0000e+00, 1.5987e-08, 2.6886e-08, 1.8555e-08, 1.2870e-08],\n",
       "        [2.5492e-09, 2.6981e-08, 4.7825e-08, 3.4117e-08, 1.7306e-08],\n",
       "        [2.7717e-09, 1.6132e-08, 3.1019e-08, 2.1887e-08, 1.2364e-08]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Passing the non-linear activations through some layers to see if they decrease to zero.\n",
    "x = torch.randn(200, 100)#Initializing the input matrix\n",
    "for i in range(50): #passing through 50 layers\n",
    "    x = relu(x @ (torch.randn(100,100) * 0.1))#Multiplying by weight matrix scaled by 0.1 and\n",
    "    #relu activated\n",
    "x[0:5,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we have the activations nearly zero.Remember about the scaling rule,the research had used\n",
    "tanh as the activation function and not ReLU.In case of ReLU,some more research was conducted and\n",
    "the right scale was calculated as sqrt(2/Nin) where Nin are the number of inputs.Let's scale the\n",
    "model according to this rule.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2721, 0.6038, 0.0147, 0.0000],\n",
       "        [0.0000, 0.3515, 0.5730, 0.0000, 0.0000],\n",
       "        [0.0000, 0.2293, 0.5150, 0.0621, 0.0000],\n",
       "        [0.0000, 0.2389, 0.7302, 0.0000, 0.0000],\n",
       "        [0.0000, 0.7179, 1.2054, 0.0035, 0.0000]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling the ReLU activated model with different rule.\n",
    "x = torch.randn(200, 100)#initializing inputs\n",
    "for i in range(50): #through the multiple layers\n",
    "    x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))#relu activation and weight matrix scaled \n",
    "    #by sqrt(2/n)\n",
    "x[0:5,0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still has some non-zero activations and not all values are zero here.So let's use this rule\n",
    "to initialize our weight matrices this time.As usual we would also initialize the input and \n",
    "output matrix again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing inputs and outputs again\n",
    "x = torch.randn(200, 100)\n",
    "y = torch.randn(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing weight matrix and bias vectors scaled using the sqrt(2/Nin) rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the weight matrices and bias vectors\n",
    "w1 = torch.randn(100,50) * sqrt(2 / 100)#scaled by sqrt(2/100)\n",
    "b1 = torch.zeros(50)#bias for layer1\n",
    "w2 = torch.randn(50,1) * sqrt(2 / 50#scaled by sqrt(2/50)\n",
    "b2 = torch.zeros(1)#bias for layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass the weight,input and bias vector through the first linear layer.Then we pass the \n",
    "returned linear activations through relu function.Next we take the mean and standard deviation of\n",
    "the relu activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5567), tensor(0.8187))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the activations\n",
    "l1 = lin(x, w1, b1)#Linear layer(input,weight,bias)\n",
    "l2 = relu(l1)#relu activation\n",
    "l2.mean(), l2.std()#mean and standard deviation of relu activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems better than the previous ones.So let's just define our model together.We define a \n",
    "function model which takes x as input,passes it through linear layer.Then passes the linear \n",
    "activations through relu layer and the relu activations are again passed through linear layer...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whole model definition\n",
    "def model(x):\n",
    "    l1 = lin(x, w1, b1)#first linear layer\n",
    "    l2 = relu(l1)#relu activatons\n",
    "    l3 = lin(l2, w2, b2)#second linear layer\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the part of the forward pass where output is computed through various layers when input\n",
    "is supplied to the model.Next we have to calculate loss from the output predictions and the \n",
    "labels.Since these are random numbers we would be using the mean squared error.Let's pass the \n",
    "input through the final model definition now to get the final predictions.\n",
    "\n",
    "We may not get the targets and labels of same shape though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Passing inputs through the whole model definition\n",
    "out = model(x)\n",
    "out.shape#shape of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels are 1d vector consisting of 200 values.But the output we got from the model is of \n",
    "shpe 200 X 1.Let's remove the extra dimension from the output using the squeeze method.Thus next\n",
    "we define a function mse through which we pass the outputs and the targets and we return the \n",
    "squezzed output and target differences squared and then taken average of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the mse function for loss \n",
    "def mse(output, targ): #pass outputs and tarets\n",
    "    return (output.squeeze(-1) - targ).pow(2).mean()#return the squared difference mean between \n",
    "#targets and the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass the output and the random output labelling vector y through mse function to calculate\n",
    "the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating loss by passing the outputs and labels \n",
    "loss = mse(out, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have calculated loss and done all the steps till the forward pass.Let's calculate \n",
    "gradients and look at the backward pass now.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients and the Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously also we have seen the use of PyTorch's Autograd package for calculating the gradients\n",
    "using loss.backward().Let's see it in terms of mathematics..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to calculate the gradients of the loss with respect to the weights of the model.We would\n",
    "thus use chain rule here as we have multiple variables.Chain rule is the calculus which tells how\n",
    "can we compute the derivative of a composite function..\n",
    "  $$(g \\circ f)'(x) = g'(f(x)) f'(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is also composed of different functions,mean squared error,the second linear layer,a ReLU \n",
    "layer and a linear layer.If the gradient of the loss with respect to b2 then loss can be defined\n",
    "as:-\n",
    "    \n",
    "    loss = mse(out,y) = mse(lin(l2, w2, b2), y)\n",
    "\n",
    "The chain rule thus says that:-\n",
    "    dloss/db2=dloss/dout X dout/db2=d(mse(out,y))/dout X d(lin(l2,w2,b2))/db2\n",
    "\n",
    "For calculating loss with respect to b2,we calculate gradient of loss with respect to output and\n",
    "multiply it with gradient of output with respect to b2.To compute all these we would have to \n",
    "calculate gradients of loss with respect to b1,w1 and b2,w2 also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate gradients,we start from the output and then go backwards towards the first layer,\n",
    "starting from last layer.That is called Backpropogation.It can be implemented using backward \n",
    "method for all the functions we used like relu,mse and lin.Thus next we define a function \n",
    "mse_grad which calculates the gradients of the loss with respect to the output of the model.That\n",
    "is actually input to loss function.Formula for derivative of x^2 that is 2x is used to calculate \n",
    "the gradient.Derivative of mean is 1/n therefore the gradient is divided by the number of \n",
    "elements in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calculating gradient of loss with respect to output of the previous layer\n",
    "def mse_grad(inp, targ): #inputs and targets passed\n",
    "    # grad of loss with respect to output of previous layer\n",
    "    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]#2 X(inp-target)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next gradients of ReLU and linear layer,gradients of loss with respect to output(out.g) and also\n",
    "apply chain rule to calculate the gradients of the loss with respect to output.Chain rule states\n",
    "that inp.g=relu'(inp) X out.g. The gradient of ReLU is either 0 or 1.So ut can be easily \n",
    "calculated by checking the positive values in input and multiplying the number with out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calculating gradient of ReLU with respect to the output of the last layer\n",
    "def relu_grad(inp, out):\n",
    "    # grad of relu with respect to input activations\n",
    "    inp.g = (inp>0).float() * out.g#checking for values>1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as to calculate the gradients of the loss with respect to input,weights,bias in the linear \n",
    "layer we define a function lin_grad through which we pass input,output,weights and bias in the\n",
    "linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calculating gradient of linear layer with respect to weight,bias and the inputs.\n",
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = inp.t() @ out.g\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed all the calculus behind the forward pass and backward pass to understand it well.\n",
    "It's not necessary to remember all the formulas..Let's discuss about some interesting thing \n",
    "called SymPy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: SymPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SymPy is a symbolic computational libray mainly used for calculus.Let us install it first using\n",
    "pip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sympy\n",
      "  Downloading sympy-1.7-py3-none-any.whl (5.9 MB)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.1.0.tar.gz (512 kB)\n",
      "Building wheels for collected packages: mpmath\n",
      "  Building wheel for mpmath (setup.py): started\n",
      "  Building wheel for mpmath (setup.py): finished with status 'done'\n",
      "  Created wheel for mpmath: filename=mpmath-1.1.0-py3-none-any.whl size=532240 sha256=d46aec77d639abbc20d7736d821f97856670951b8a183b8bd8760ed8296a4458\n",
      "  Stored in directory: c:\\users\\kruti\\appdata\\local\\pip\\cache\\wheels\\29\\2c\\1c\\d2e4580cde2743b0aef389e936ac21a2db92921ddbca53faa1\n",
      "Successfully built mpmath\n",
      "Installing collected packages: mpmath, sympy\n",
      "Successfully installed mpmath-1.1.0 sympy-1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install sympy library using pip\n",
    "pip install sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sybmolic computation,,first we define a symbol and then calculation happens.Let's see how\n",
    "sympy works.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 sx$"
      ],
      "text/plain": [
       "2*sx"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import symbols and differentiation from sympy\n",
    "from sympy import symbols,diff\n",
    "sx,sy = symbols('sx sy')#get the symbols\n",
    "diff(sx**2, sx)#differentiate the expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus sympy has calculated the derivative of sx**2 as 2sx.It can take more complex and compound\n",
    "expressions and equations.Most of the things we saw in this chapter are manually done using user.\n",
    "But PyTorch and now sympy allows us to calculate gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End sidebar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hav discussed about forward pass and the backward pass in details.We define separate function\n",
    "for forward and bakward pass.The gradients are not stored anywhere instead it is just executed \n",
    "in reverse to that of forward pass.Let's define the \"forward_and_backward\" function which would\n",
    "take input and targets.We should note that loss is not required in backward pass.\n",
    "\n",
    "In the forward pass,we pass the input through first layer,the ReLU activation and then passing\n",
    "through the next linear layer.In the backward pass,we simply call out the mse_grad functions for \n",
    ",the linear layer,the lin_gred for linear gradient and ReLU for ReLU gradient.We had previously\n",
    "defined the gradient functions for different layers.This way even we can access all the model\n",
    "parameters w1.g,W2.g,b1.g.,b2.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for forward pass and backward pass\n",
    "def forward_and_backward(inp, targ):#Passing inputs and targets\n",
    "    # forward pass:\n",
    "    \n",
    "    l1 = inp @ w1 + b1#first linear layer\n",
    "    \n",
    "    l2 = relu(l1)#ReLU activation\n",
    "    \n",
    "    out = l2 @ w2 + b2#second linear layer\n",
    "    # we don't actually need the loss in backward!\n",
    "    loss = mse(out, targ)\n",
    "    \n",
    "    # backward pass:\n",
    "    mse_grad(out, targ)#gradient for loss\n",
    "    lin_grad(l2, out, w2, b2)#gradient for first linear layer\n",
    "    relu_grad(l1, l2)#ReLU activation\n",
    "    lin_grad(inp, l1, w1, b1)#gradient for second linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the model like a PyTorch module.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model we used three functions and then 2 associated functions,forward pass and backward \n",
    "pass with them.We had written the functions separately but and then called them whenever it was\n",
    "required.This can be made simpler and modified by creating a class for that.The class would store\n",
    "inputs and outputs for backward pass and we would have to call backward only.We would create 3 \n",
    "different classes for Relu,Linear and Mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class for Relu layer\n",
    "class Relu():\n",
    "    def __call__(self, inp):#input\n",
    "        self.inp = inp#input\n",
    "        self.out = inp.clamp_min(0.)#clamped input=output\n",
    "        return self.out#return output\n",
    "    \n",
    "    def backward(self): \n",
    "        self.inp.g = (self.inp>0).float() * self.out.g#gradient for Relu layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"__call__\" is known as the magic name in Python which makes class callable.It will be executed \n",
    "when y=Relu() is instantiated.In the same way we create classes for Linear layer and mse loss\n",
    "also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for linear layer\n",
    "class Lin():\n",
    "    def __init__(self, w, b): #input paramaters\n",
    "        self.w,self.b = w,b\n",
    "        \n",
    "    def __call__(self, inp):#magic command executed when class instantiated\n",
    "        self.inp = inp#input\n",
    "        self.out = inp@self.w + self.b#output\n",
    "        return self.out#return output\n",
    "    \n",
    "    def backward(self):#gradient calculation steps\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for Mean Squared error(Loss function)\n",
    "class Mse():\n",
    "    def __call__(self, inp, targ):#inout and targets passed\n",
    "        self.inp = inp#inputs\n",
    "        self.targ = targ#target\n",
    "        self.out = (inp.squeeze() - targ).pow(2).mean()#outout calculation\n",
    "        return self.out#return output\n",
    "    \n",
    "    def backward(self):#gradient calculation\n",
    "        x = (self.inp.squeeze()-self.targ).unsqueeze(-1)\n",
    "        self.inp.g = 2.*x/self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can put everything together in model like last time but this time in class and not as \n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model class together including the forward pass and backward pass\n",
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):#weights and bias passed\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]#list of layers\n",
    "        self.loss = Mse()#loss function\n",
    "        \n",
    "    def __call__(self, x, targ):#executed when the class is called\n",
    "        for l in self.layers:#iterating through all layers\n",
    "            x = l(x)#pass the input through layers,store output\n",
    "        return self.loss(x, targ)#return the loss calculated through loss by passing output and\n",
    "                                 #targets\n",
    "    \n",
    "    def backward(self):#function for backward pass\n",
    "        self.loss.backward()#calculate gradient for loss function\n",
    "        for l in reversed(self.layers): #iterating through reversed layers\n",
    "            l.backward()#compute gradient for each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create an object for the model by instantiating Model class and pass weights and bias for\n",
    "both layers through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating model\n",
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass executed by calling the model object and passing x(inputs) and y(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward pass in the model\n",
    "loss = model(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And backward pass is executed using model.backward() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward pass in the model\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we had written all the function classes ReLU,mse and linear separately.Next we\n",
    "would create such class that all the function classes are included in it and it will inherit from\n",
    "the base class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ReLu,mse and linear classes we created earlier are very much similar so we create a \n",
    "parent class and make these three inherit from that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create LayerFunction as base class.And we create forward and backward(bwd)functions so as to\n",
    "implement forward and backward class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating base class to inherit the function classes\n",
    "class LayerFunction():\n",
    "    def __call__(self, *args):#arguments\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)#arguments passed in forward pass\n",
    "        return self.out#return output of forward pass\n",
    "    \n",
    "    def forward(self): #forward pass \n",
    "        raise Exception('not implemented')\n",
    "    def bwd(self):      \n",
    "        raise Exception('not implemented')\n",
    "    def backward(self): #backward pass\n",
    "        self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the Linear,MSE and Relu classes we had written earlier..But this time they would\n",
    "be inherited from the LayerFunction base class.We would implement the forward and bwd methods in \n",
    "each of the inherited classes here.But this time they would be inherited from the LayerFunction \n",
    "base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inherited class for Relu\n",
    "class Relu(LayerFunction):#inherited from layerFunction class\n",
    "    def forward(self, inp): #input passed\n",
    "        return inp.clamp_min(0.)#returns the clamped input at zero as output\n",
    "    def bwd(self, out, inp):#pass input,output through backward \n",
    "        inp.g = (inp>0).float() * out.g#calculate the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inherited class for Linear layer\n",
    "class Lin(LayerFunction):#inherited from LayerFunction class\n",
    "    def __init__(self, w, b): #pass weight and bias\n",
    "        self.w,self.b = w,b\n",
    "        \n",
    "    def forward(self, inp): \n",
    "        return inp@self.w + self.b#return Wx+b\n",
    "    \n",
    "    def bwd(self, out, inp):#pass input,output through backward\n",
    "        #calculate gradient for linear layer\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inherited class for mse(loss function)\n",
    "class Mse(LayerFunction):#inherited from LayerFunction class\n",
    "    def forward (self, inp, targ):#pass predictions and targets\n",
    "        return (inp.squeeze() - targ).pow(2).mean()#return(prediction-target)^2)\n",
    "    def bwd(self, out, inp, targ): #pass output,input,target\n",
    "        inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]#calculate gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have created an inherited class containing subclasses for the three functions.Rest \n",
    "part of the model is same.Every basic function to be differentiated can be written as an object\n",
    "form in autograd package of PyTorch.The object is torch.autograd.Function and has a forward and \n",
    "backward method.This leads to PyTorch keeping record of the computation and operations we perform\n",
    "on the functions.If required_grad is set to False then  it won't store about the gradients of\n",
    "the other functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the same thing in PyTorch.It is almost same as writing the earlier classes.Only \n",
    "difference it makes is that we can decide wht to choose and what to assign to the variable,the \n",
    "gradients are also returned in backward function.Let's see how can we write our own function\n",
    "object though very rarely we would need this.Let'see how we can write that.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating function object \n",
    "from torch.autograd import Function\n",
    "\n",
    "class MyRelu(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i.clamp_min(0.)\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i, = ctx.saved_tensors\n",
    "        return grad_output * (i>0).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above definition is not very often used.Instead for more complex models we use Function \n",
    "objects in torch.nn Module.It provides the starting point for all models aand includes all the\n",
    "neural nets we had studied till now.Main feature of this module is that it allows us to initiali-\n",
    "ze the trainable parameters on its own. \n",
    "\n",
    "For implementing nn.Module we follow following steps:-\n",
    "    \n",
    "1.The superclass \"__init__\" is the first one to be called always and it initializes the \n",
    "parameters.\n",
    "\n",
    "2.Define parameters for the model amd store them in nn.parameters\n",
    "\n",
    "3.There is a forward function also which returns the output when input is passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct a linear layer using PyTorch's torch.nn Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn #import torch.nn module\n",
    "\n",
    "class LinearLayer(nn.Module):#creating linear layer\n",
    "    def __init__(self, n_in, n_out):#superclass\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in))#initialize weight\n",
    "    \n",
    "        self.bias = nn.Parameter(torch.zeros(n_out))#Initialize bias vector \n",
    "    \n",
    "    def forward(self, x):#forward function\n",
    "        return x @ self.weight.t() + self.bias#return Wx+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used nn.Parameter to define the parameters the class keeps a record of that.We can \n",
    "easily access them using object.parameters()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10]), torch.Size([2]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accessing the parameters of the linear layer\n",
    "lin = LinearLayer(10,2)#Instantiate the object\n",
    "p1,p2 = lin.parameters()#store the parameters\n",
    "p1.shape,p2.shape#shape of the weights and bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's nn.Module also lets us update the parameters using just opt.step().In PyTorch the\n",
    "weights are stored as n_out X n_in matrix.That's why in the forward pass there is a w.t() that is\n",
    "transpose step to make the dimensions compatible.So instead of initializing parameters differently we add PyTorch's Linear,ReLU and Linear layer in sequence using PyTorch's nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using PyTorch's nn.Module layers to build the model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):#Pass number of inputs,hidden states and number of output\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(#define the sequential layers\n",
    "            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n",
    "        self.loss = mse#loss function\n",
    "        \n",
    "    def forward(self, x, targ):#forward pass \n",
    "        return self.loss(self.layers(x).squeeze(), targ)#calculating the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastai has a replacement as Module instead of nn.Module and we don't need to call super().init()\n",
    "here which we were doing in case of nn.Module.The rest of the steps and code remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using PyTorch's Module layers to build the model\n",
    "class Model(Module):\n",
    "    def __init__(self, n_in, nh, n_out):#Pass number of inputs,hidden states and number of output\n",
    "        self.layers = nn.Sequential(#define the sequential layers\n",
    "            nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))\n",
    "        self.loss = mse#loss function\n",
    "        \n",
    "    def forward(self, x, targ): #forward pass \n",
    "        return self.loss(self.layers(x).squeeze(), targ)#calculating the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next chapter we would start from this model and learn about modifying the training loop from \n",
    "scratch and refactoring it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter we concentrated on building forward and backward steps of training loop through \n",
    "different ways.We started from very basics of deep learning,matrix multiplication and step by \n",
    "step reached towards implementing forward and backward passes by scratch.We also used pure \n",
    "PyTorch to build the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important takeaways of this chapter are as follows:-\n",
    "\n",
    "1.A neural net consists of matrix multiplications with non linear layers in between.\n",
    "\n",
    "2.We use techniques such as elementwise operations and broadcasting for faster operations in \n",
    "Python.\n",
    "\n",
    "3.Initializing a neural net is very important with proper scaling else the activations value may\n",
    "not be proper.\n",
    "\n",
    "4.Backward pass in the training process involves calculating gradients many times using chain \n",
    "rule.\n",
    "\n",
    "5.When we try to subclass the PyTorch's nn.Module calling superclass of \"__init__\" method inside \n",
    "the function is compulsory and a forward function has to be there which takes input and gives the\n",
    "predictions after passing the input through layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write the Python code to implement a single neuron.\n",
    "1. Write the Python code to implement ReLU.\n",
    "1. Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "1. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).\n",
    "1. What is the \"hidden size\" of a layer?\n",
    "1. What does the `t` method do in PyTorch?\n",
    "1. Why is matrix multiplication written in plain Python very slow?\n",
    "1. In `matmul`, why is `ac==br`?\n",
    "1. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "1. What is \"elementwise arithmetic\"?\n",
    "1. Write the PyTorch code to test whether every element of `a` is greater than the corresponding element of `b`.\n",
    "1. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "1. What does this return, and why? `tensor([1,2]) + tensor([1])`\n",
    "1. What does this return, and why? `tensor([1,2]) + tensor([1,2,3])`\n",
    "1. How does elementwise arithmetic help us speed up `matmul`?\n",
    "1. What are the broadcasting rules?\n",
    "1. What is `expand_as`? Show an example of how it can be used to match the results of broadcasting.\n",
    "1. How does `unsqueeze` help us to solve certain broadcasting problems?\n",
    "1. How can we use indexing to do the same operation as `unsqueeze`?\n",
    "1. How do we show the actual contents of the memory used for a tensor?\n",
    "1. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)\n",
    "1. Do broadcasting and `expand_as` result in increased memory use? Why or why not?\n",
    "1. Implement `matmul` using Einstein summation.\n",
    "1. What does a repeated index letter represent on the left-hand side of einsum?\n",
    "1. What are the three rules of Einstein summation notation? Why?\n",
    "1. What are the forward pass and backward pass of a neural network?\n",
    "1. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "1. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "1. How can weight initialization help avoid this problem?\n",
    "1. What is the formula to initialize weights such that we get a standard deviation of 1 for a plain linear layer, and for a linear layer followed by ReLU?\n",
    "1. Why do we sometimes have to use the `squeeze` method in loss functions?\n",
    "1. What does the argument to the `squeeze` method do? Why might it be important to include this argument, even though PyTorch does not require it?\n",
    "1. What is the \"chain rule\"? Show the equation in either of the two forms presented in this chapter.\n",
    "1. Show how to calculate the gradients of `mse(lin(l2, w2, b2), y)` using the chain rule.\n",
    "1. What is the gradient of ReLU? Show it in math or code. (You shouldn't need to commit this to memory—try to figure it using your knowledge of the shape of the function.)\n",
    "1. In what order do we need to call the `*_grad` functions in the backward pass? Why?\n",
    "1. What is `__call__`?\n",
    "1. What methods must we implement when writing a `torch.autograd.Function`?\n",
    "1. Write `nn.Linear` from scratch, and test it works.\n",
    "1. What is the difference between `nn.Module` and fastai's `Module`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement ReLU as a `torch.autograd.Function` and train a model with it.\n",
    "1. If you are mathematically inclined, find out what the gradients of a linear layer are in mathematical notation. Map that to the implementation we saw in this chapter.\n",
    "1. Learn about the `unfold` method in PyTorch, and use it along with matrix multiplication to implement your own 2D convolution function. Then train a CNN that uses it.\n",
    "1. Implement everything in this chapter using NumPy instead of PyTorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers:-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-1 def lin(x,w,b):\n",
    "        return x@w+b\n",
    "\n",
    "Ans-2 def relu(x): \n",
    "    return x if x >= 0 else 0\n",
    "\n",
    "Ans-3 sum([x*w for x,w in zip(input,weight)])\n",
    "\n",
    "Ans-4 y[i,j] = sum([a * b for a,b in zip(x[i,:],w[j,:])]) + b[j]\n",
    "\n",
    "Ans-5 Hidden size is the number of neurons in a layer in a neural net and all the inputs are \n",
    "linked to each neuron.\n",
    "\n",
    "Ans-6 It takes the transpose in PyTorch\n",
    "\n",
    "Ans-7 It is because plain Python uses for loops for matrix multiplication which are very slow.\n",
    "PyTorch is written in C++ which is much faster.\n",
    "\n",
    "Ans-8 In case of matrix multiplication between 2-D matrices a and b in order to have the matrices\n",
    "compatible a's columns should be equal to b's rows.\n",
    "\n",
    "Ans-9 using the magic command %time\n",
    "\n",
    "Ans-10 All the mathematical operators can be applied elementwise on matrices.If two tensors a and\n",
    "b are of same shape and size,then operations are performed between corresponding elements at the \n",
    "similar positions in both the matrices.\n",
    "\n",
    "Ans-11 (a<b)\n",
    "\n",
    "Ans-12 Rank-0 tensors are tensors containing a single element..item() can be used to convert it \n",
    "into a plain Python number.\n",
    "\n",
    "Ans-13 tensor([2,3]).tensor([1]) is extended to the size of [1,2] and then addition takes place\n",
    "using broadcasting.\n",
    "\n",
    "Ans-14 Error as the sizes do not match so broadcasting is not possible\n",
    "\n",
    "Ans-15 Using elementwise arithmetic we can remove one of the nested loops,the tensors can be \n",
    "multiplied directly before summing up.The inner most loop is then replaced by PyTorch elementwise\n",
    "arithmetic.\n",
    "\n",
    "Ans-16 Two dimensions are said to be compatible for broadcasting only when:-\n",
    "1.They are equal.\n",
    "2.If either is 1 then it can be expanded to match the other.\n",
    "\n",
    "Ans-17 expand_as method is used to expand a vector to match the size of other tensor of more\n",
    "dimensions.\n",
    " \n",
    "Ans-18-Broadcasting is done along rows most of the times but to do it along any other dimension\n",
    "the vector shape can be changed by adding a unit axis to the vector.It can be done using \n",
    "unsqueeze method.\n",
    "\n",
    "Ans-19 Unsqueezed method can be replaced using None indexing.To add an extra unit dimension,we\n",
    "add None in that dimension while indexing the tensors.a[None,:]\n",
    "\n",
    "Ans-20 using storage method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-21 Elements of vector are added to each row of matrix by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-22 No the storage space remains same.It is because in whatever dimension broadcasting occurs\n",
    "it has a stride of 0 that is PyTorch looks for next row by adding stride and doesn't move.\n",
    "\n",
    "Ans-23 def matmul(a,b): \n",
    "    return torch.einsum('ik,kj->ij', a, b)\n",
    "\n",
    "Ans-24 Summing over takes place along that repeated axis.\n",
    "\n",
    "Ans-25 Einstein summation rules are as follows:-\n",
    "\n",
    "1.Repeated indices is summed over.\n",
    "\n",
    "2.Every index can occur maximum twice\n",
    "\n",
    "3.The term must contain identical non-repeated indices.\n",
    "\n",
    "Ans-26 While training a model, the process is divided into two parts,forward pass is when output \n",
    "is calculated through the model from given input and in backward pass we calculate gradients of\n",
    "loss with respect to parameters to update the parameters.\n",
    "\n",
    "Ans-27 It is because they are required for calculating gradients during the backward pass.\n",
    "\n",
    "Ams-28 If the activations have a standard deviation far away from 1 as if there are many layers,\n",
    "then every layer will multiply the scale of the activations by 10 and at the end the number would\n",
    "be so large that we won't be able to store it in computers.\n",
    "\n",
    "Ans-29 Weight initialization results in rescaling of the weights thus the activations be in the \n",
    "specific range and there are less zeros also.\n",
    "\n",
    "Ans-30 sqrt(2/n)\n",
    "\n",
    "Ans-31 squeeze method is used for removing extra unit dimension.In loss function we are required\n",
    "to calculate the difference between the targets and outputs from model.Sometimes,the output has\n",
    "one extra dimension so as to get rid of that squeeze is used.\n",
    "\n",
    "Ans-32 It refers to the index  of the dimension to be removed.The index can be considered to be \n",
    "the axis index in the shape of the matrix.\n",
    "\n",
    "Ans-33 Chain rule is a calculus rule used for calculating the derivative of a composite function.\n",
    "dy/dx = dy/du * du/dx\n",
    "\n",
    "Ans-35 0 or 1.(inp >0).float().out.g\n",
    "\n",
    "Ans-36 loss_grad,linear_grad,relu_grad,linear_grad(reverse order of layers)\n",
    "\n",
    "Ans-37 A magic name in Python makes class collable.\n",
    "\n",
    "Ans-38 Forward and backward\n",
    "\n",
    "Ans-40 nn.Module requires call to the superclass init after defining it.Fastai's Module doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
