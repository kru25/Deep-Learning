{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter we learnt about the different transforms,collections for data transformation,\n",
    "pipeline of transformations for both Image Data and Text Data using fastai's mid-level data API apart\n",
    "from DataBlock API.Till now whatever we learnt about different models was mostly through fastai's \n",
    "customized objects,methods abd functions.Now we would be going into little deep in deep learning.This\n",
    "part of the course would focus on building state of art models.\n",
    "This Chapter will stress on fine-tuning pretrained language model to build text classifier.We would\n",
    "also learn about building a RNN and LSTMs from scratch.Let's start from Language Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Language Model from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be implementing a language model from scratch for first time,we would be using a simple\n",
    "dataset here for easy and quick interpretation of results.We would be using Human Numbers dataset,and\n",
    "they are simply 10000 numbers written in English like one,two,three..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download,extract and see what are the folders and contents of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading,extracting datasets and assigning it to path\n",
    "from fastai.text.all import *\n",
    "path = untar_data(URLs.HUMAN_NUMBERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('train.txt'),Path('valid.txt')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ls method to see the contents of the dataset\n",
    "path.ls()\n",
    "#contains training and validation folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ls() method used earlier we can see that there are two files \"train.txt\" and \"valid.txt\" for \n",
    "training and validation data respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files contain the numbers written in text format in different lines.We open the files and \n",
    "concatenate all the texts together by ignoring the training,validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a List Object to store the concatenated text  \n",
    "lines = L()\n",
    "#opening the training and validation file and reading the text from it and storing it in the List \n",
    "#object\n",
    "with open(path/'train.txt') as f: lines += L(*f.readlines())\n",
    "with open(path/'valid.txt') as f: lines += L(*f.readlines())\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next all the lines are taken and concatenated into big doc.\".\" is used as separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' . '.join([l.strip() for l in lines]) #Joining word in each line using \".\" separator\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tokenize the words by splitting on spaces.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = text.split(' ') #Tokenizing the words\n",
    "tokens[:10]\n",
    "#\".\" are also considered as separate tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes numericalization.For that we create vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating list of unique tokens(vocab)\n",
    "vocab = L(*tokens).unique()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for numericalization we look up the index of each word in the vocab.And then in the list of tokens\n",
    "those words are replaced by the index of the particular word in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#63095) [0,1,2,1,3,1,4,1,5,1...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = {w:i for i,w in enumerate(vocab)} #converting words into indexes(numbers) in vocab\n",
    "nums = L(word2idx[i] for i in tokens) #Numericalizing the tokens using the index pf each word in vocab\n",
    "nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done the two main tasks of the text processing(tokenization,numericalization) for a Language \n",
    "model so our dataset is ready now.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our First Language Model from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be building a language model that would predict next word on the basis of last three words.\n",
    "For creating dataloaders we need list of tuples containing independent and dependent variables.So\n",
    "here our independent variable is a sequence of three words and the dependent variable is the next word\n",
    "in the sequence.We create tuples for all possible combinations of words.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-10-a277f876a917>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-a277f876a917>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3)\u001b[0m\n\u001b[1;37m                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#Creating list Object containing tuples of independent and dependent word pairs by iterating over\n",
    "#pair of every three words in the list of tokens using for loop\n",
    "L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code cell the list of tuples was created for word tokens.Deep learning models work \n",
    "with numerical data only so we create these list of tuples for numericalized tokens using the tensors \n",
    "of the numericalized tokens by of taking all possible sequences of three numbers in the list.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10,  1, 11]), 1),(tensor([ 1, 12,  1]), 13),(tensor([13,  1, 14]), 1),(tensor([ 1, 15,  1]), 16)...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3))\n",
    "seqs\n",
    "#In the first tuple [0,1,2] is the input and 1 is the label.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data into Dataloaders into minibatches as it is arranged separately into x,y \n",
    "pairs.The training and validation split is done randomly by using 80% for training and rest 20% for\n",
    "validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64 #Setting batch size\n",
    "cut = int(len(seqs) * 0.8) #index for random splitting into training  and validation set\n",
    "dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False,num_workers=0) \n",
    "#Creating DataLoaders by passing batch size,training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow so we have created a Dataloaders object also so we ready to create our first language model in \n",
    "Pytorch from scratch..\n",
    "\n",
    "We would create a neural network that would take three words as input and return the probability of \n",
    "prediction for each word in the vocab.It is similar to multicategory classification we had done \n",
    "earlier.\n",
    "\n",
    "We will be constructing a three layer model with some modifications.First layer will have first word's\n",
    "embeddings as activations,second layer will have second layer's embedding and first layer's output \n",
    "activations and third layer will have third word's embedding plus second layer's output as activations\n",
    ".This is done so that every word is related to the word preceding it.\n",
    "\n",
    "All these layers have same weight matrix that is the activations won't depend on the position of the \n",
    "word.Though activations will change as data moves from layer to layer but the layer weights won't \n",
    "change.So each layer will indirectly learn about all positions in the sequence.\n",
    "Since our model is sequential so layers can be called repeated.Pytorch allows us to create one layer\n",
    "and then repeat it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write architecture for the model now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Language Model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create class for the Language Model 1(LMModel1)Pytorch module is passed so that we can use all the \n",
    "#layer modules included in that\n",
    "class LMModel1(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        #Input Layer\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)#First layer(size of the vocab,batch_size)#passed\n",
    "        #while creating learner \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)#Hidden layer(Same as previous layer,Same as previous\n",
    "        #layer)       \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)#Final layer(Same as previous layer,vocab size(output))\n",
    "        #Output Layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.h_h(self.i_h(x[:,0])))#Relu activation for first layer by passing first word\n",
    "        #embedding\n",
    "        h = h + self.i_h(x[:,1])#second word input\n",
    "        h = F.relu(self.h_h(h)) #Activation of the second word \n",
    "        h = h + self.i_h(x[:,2])#Third word input \n",
    "        h = F.relu(self.h_h(h))#Activation of the third word\n",
    "        return self.h_o(h) #Output layer predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Neural network above three layers are created:\n",
    "1.The Embedding layer(i_h)(input layer) input to hidden layer\n",
    "2.The Linear layer creates activation for next word(h_h)(hidden layer) \n",
    "3.Final Layer to predict the fourth word(h_o) (output layer)\n",
    "\n",
    "It can also be represented pictorially using the below flow chart..\n",
    "<img alt=\"Pictorial representation of simple neural network\" width=\"400\" src=\"images/att_00020.png\" caption=\"Pictorial representation of a simple neural network\" id=\"img_simple_nn\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure rectangle represents the input layer,circle represents the inner layer and the\n",
    "triangle represents the final layer.The above figure represents the basic architecture.As mentioned \n",
    "before Since the layers have same embedding matrix and weights so Pytorch allows us to create one \n",
    "layer and then use the same again.The actual model looks like following:-\n",
    "<img alt=\"Representation of our basic language model\" width=\"500\" caption=\"Representation of our basic language model\" id=\"lm_rep\" src=\"images/att_00022.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the above figure first word is fed to the input layer.Then it is passed through the hidden layer\n",
    "to get activations which serve as the input to the second hidden layer along with the second word \n",
    "input.The activations from the second hidden layer along with the input from third word are fed to\n",
    "third hidden layer which are then fed to the output layer which predicts the next word in the sequence\n",
    ".Now we can create learner through this model by passing the dataloaders,the model object(vocab size,\n",
    "batch-size),Loss function and the metrics.We further train the model for 4 epochs and a learning rate \n",
    "of \"10^-3\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.824297</td>\n",
       "      <td>1.970941</td>\n",
       "      <td>0.467554</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.386973</td>\n",
       "      <td>1.823242</td>\n",
       "      <td>0.467554</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.417556</td>\n",
       "      <td>1.654497</td>\n",
       "      <td>0.494414</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.376440</td>\n",
       "      <td>1.650849</td>\n",
       "      <td>0.494414</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating the learner for the model\n",
    "learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, \n",
    "                metrics=accuracy)\n",
    "learn.fit_one_cycle(4, 1e-3)#Training the model for 4 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we have trained so far is a very basic model with only three layers and therefore is \n",
    "considered a baseline model for our final predictions.Now we would try to train a model which can \n",
    "give better accuracy than this.\n",
    "Let's see which token occurs most in the labels of the validation set.The simple baseline model will\n",
    "predict the most common token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(29), 'thousand', 0.15165200855716662)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,counts = 0,torch.zeros(len(vocab))\n",
    "for x,y in dls.valid:\n",
    "    n += y.shape[0]\n",
    "    for i in range_of(vocab): counts[i] += (y==i).long().sum()\n",
    "idx = torch.argmax(counts)\n",
    "idx, vocab[idx.item()], counts[idx].item()/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common token has the index of 29 which stands for word \"thousand\".It is predicted with an accuracy\n",
    "of 15%.It is most common as when we write the words for numbers till 10000,starting from one thousand,\n",
    "this word comes in every number and therefore it is the most common.\n",
    "The Previous baseline model gave accuracy of 49% which is good as a baseline model.Let's build another\n",
    "model using some refactoring.The number and type of layers would remain same,only here we will be \n",
    "iterating in a for loop to call the hidden layer and input layer for each word.This type of model is \n",
    "called \"Recurrent Neural Network\".Let's first create the architecture for the same.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our First Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of layers and the architecture of this model is same as the Baseline model.We will refactor\n",
    "it using a for loop and the advantage it provides is that it can take varying number of words in the \n",
    "independent variable set.Therefore it will accept list of more than 3 words also.Let's create the \n",
    "model now.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel2(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):#(vocab size,batch size)\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)#Input layer (vocab size,batch size)\n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)#Hidden Layer (same as the previous layer,)\n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)#output layer(same as previous layer,predictions are\n",
    "        #equal to vocab size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = 0\n",
    "        for i in range(3):#iterating over the three words in the input\n",
    "            h = h + self.i_h(x[:,i])#passing the word through the input layer\n",
    "            h = F.relu(self.h_h(h))#getting activations through the hidden layer\n",
    "        return self.h_o(h)#returning the final predictions after iterating through all the words in \n",
    "   #the input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our model by creating learner and passing the model architecture.Then we train the model\n",
    "for 4 epochs and a learning rate of 10^-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.816274</td>\n",
       "      <td>1.964143</td>\n",
       "      <td>0.460185</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.423805</td>\n",
       "      <td>1.739964</td>\n",
       "      <td>0.473259</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.430327</td>\n",
       "      <td>1.685171</td>\n",
       "      <td>0.485382</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.388390</td>\n",
       "      <td>1.657033</td>\n",
       "      <td>0.470406</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating learner for the model\n",
    "learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, \n",
    "                metrics=accuracy)\n",
    "learn.fit_one_cycle(4, 1e-3)#Fitting the model for 4 epochs.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After refactoring the model also,we get almost same results.The accuracy remains the same.The only \n",
    "difference between the previous model and the current model is that we have included a for loop to \n",
    "iterate over the words in the input list.The activations are updated everytime through the loop and \n",
    "are stored in h.This is one of the main properties of RNNs.Any neural network defined through loop is\n",
    "called Recurrent Neural Network(RNN).It is made by refactoring multilayered neural network by \n",
    "iterating over a for loop.We trained a simple RNN model for a few epochs and got results similar to \n",
    "the baseline model.We can further improve its performance by introducing some changes.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would discuss about how we can improve the performance of a RNN..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In RNN architecture,hidden state(h variable) is initialized to zero for every sequence of words passed\n",
    "through the network.The length of the sequences(no of words in each sequence) is fixed as 3 for now.It\n",
    "is minimized so that data can easily fit in mini batches.If the sequences were ordered correctly then\n",
    "the length of the sequence would be very long on which the model will get trained then.\n",
    "One more thing to notice is that we are predicting fourth word only here but we can actually predict\n",
    "second and third word too as model learns about all the positions.\n",
    "We can introduce these modifications in the model.Let's see how.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maintaining the State of an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous model for each new sample the hidden state is initialized to 0 which tells that no \n",
    "previous information about the sentence is carried forward.This means that model is not aware about\n",
    "wheather we are present at the end of the sentence or middle or starting.This can be corrected by \n",
    "initializing the hidden state in \"__init__\" instead of \"forward\".\n",
    "When we initialize the model's hidden state to zero,it actually adds to the layers in our model.The \n",
    "number of layers are equal to the number of the number of tokens in the document.Pictorial figure of\n",
    "RNN without for loop can explain why this happens.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very first model with three layers and without for loop was the baseline model.In that model each\n",
    "layer is fed with one input token.It is also called \"unrolled representation\".In this way when we \n",
    "do refactoring using for loop and initialize the hidden state in __init__ , the model becomes deep and\n",
    "for a dataset with 10000 tokens,the model will be 10000 layered.Problem with such deep model is that\n",
    "for calculating derivatives,it will have to do it starting from first layer.This will cause model to \n",
    "be slow and also consume a lot of memory.\n",
    "To avoid this,backpropogation would happen only for the last three layers.Detach method is used to\n",
    "remove the gradient history in Pytorch.\n",
    "The new variant of the RNN is as follows.It has h initialized in __init__ and it also remembers each\n",
    "activation while calling forward everytime so it is used for different samples in the mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel3(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)#Input layer  \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)#Hidden layer     \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)#Output Layer\n",
    "        self.h = 0#Initializing hidden state to zero\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(3):#iterating over 3 words in each sample\n",
    "            self.h = self.h + self.i_h(x[:,i])#passing word through input layer\n",
    "            self.h = F.relu(self.h_h(self.h))#Activation through hidden layer\n",
    "        out = self.h_o(self.h)#Output predicted through output layer\n",
    "        self.h = self.h.detach()#detaching the gradients calculated for previous layer\n",
    "        return out #returning output\n",
    "    \n",
    "    def reset(self): self.h = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every sample length,this model has same activations as this model remembers the activations from \n",
    "previous batch also.This solves the problem of not learning about any previous information about \n",
    "preceding batch.The gradients would be calculated for only the three tokens in the sequence and this \n",
    "method is called \"backpropogation through time\"(BPTT).This involves training each layer in neural net \n",
    "in one time step to save memory and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order of the data matters a lot in the LMModel3.So data should be passed in order such that dset[0] is\n",
    "the first line in first batch,dset[1] is the first line in second batch and so on.\n",
    "To ensure this data has to be arranged.For that data is divided into equal number of m=len(seqs)//bs \n",
    "sampled groups where bs is batch-size=64 and len(seqs) is total data size and m is length of each part\n",
    ".Now the data would be divided into batches such that first batch consists of samples as:-\n",
    "    \n",
    "(0,m,2*m,........(bs-1)*m)\n",
    "\n",
    "and the second batch would have:-\n",
    "\n",
    "(1,m+1,2*m+1,....(bs-1)*m+1) and so on..\n",
    "\n",
    "so for each epoch,model will receive text of size 3*m.Since,every text is of size 3 on every line of \n",
    "batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 64, 21031)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = len(seqs)//bs #Length of each piece in batch=(Total dataset length/batch size)\n",
    "m,bs,len(seqs) #Length of each piece,batch size,Total length "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function \"group_chunks\" does reindexing by passing batch size and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for reindexing each batch.Returns the reindexed dataset\n",
    "def group_chunks(ds, bs):\n",
    "    m = len(ds) // bs #Length of each piece in batch\n",
    "    new_ds = L() #Creating new List object\n",
    "    for i in range(m): #Iterates through the piece length\n",
    "        new_ds += L(ds[i + m*j] for j in range(bs))#Reindexing for whole batch in the similar way as\n",
    "        #Previously mentioned batch contents\n",
    "    return new_ds#return the indexed batch contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build DataLoaders and for training and validation split we pass the index at which split will\n",
    "be done.\"drop_last\"=True is also passed so that last batch can be dropped since it doesn't have a \n",
    "batch size equal to bs.\"Shuffle\"=False is passed so that there is no text rearrangement and text is \n",
    "read in sequence only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataloaders from the dataset\n",
    "cut = int(len(seqs) * 0.8)#index for training,validation split\n",
    "dls = DataLoaders.from_dsets(\n",
    "    group_chunks(seqs[:cut], bs),#returns training set \n",
    "    group_chunks(seqs[cut:], bs),#returns validation set\n",
    "    bs=bs, drop_last=True, shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create learner for training the model,Since our model contains many layers so we also increase\n",
    "the number of epochs thus training model for longer.The last argument passed through learner is cbs=\n",
    "\"ModelResetter\".It is a callback as it calls the reset method in model in the begining of each epoch\n",
    "and validation phase.This ensures that before starting each epoch , the hidden set is reset for \n",
    "learning the sequential text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.677074</td>\n",
       "      <td>1.827367</td>\n",
       "      <td>0.467548</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.282722</td>\n",
       "      <td>1.870913</td>\n",
       "      <td>0.388942</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.090705</td>\n",
       "      <td>1.651794</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.005216</td>\n",
       "      <td>1.615990</td>\n",
       "      <td>0.515144</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.963020</td>\n",
       "      <td>1.605894</td>\n",
       "      <td>0.551202</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.916012</td>\n",
       "      <td>1.704552</td>\n",
       "      <td>0.547596</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.895979</td>\n",
       "      <td>1.651697</td>\n",
       "      <td>0.560577</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.837829</td>\n",
       "      <td>1.661127</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.806078</td>\n",
       "      <td>1.697939</td>\n",
       "      <td>0.594231</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.792238</td>\n",
       "      <td>1.657946</td>\n",
       "      <td>0.603125</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating learner for training model\n",
    "#Passing dataloaders,model architecture(vocab-size,batch size),loss function,metrics and cbs for \n",
    "#calling model reset.\n",
    "learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,\n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(10, 3e-3)#training the model for more number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy improves to 58% after introducing the changes such as initializing hidden state in \n",
    "\"__init__\" and using for loop for refactoring the model.This version of RNN also remembers the \n",
    "preceding sequences of words.But the model can still be improved more.Let's see how.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating More Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models we built till now were predicting next one word in sequence using the last three words.\n",
    "So for backpropogation,the information of a single word is sent to update the weights for three words\n",
    "inputs.Thi can be increased and made better by predicting  next word after every single word.\n",
    "The pictorial presentation would be like:-\n",
    "<img alt=\"RNN predicting after every token\" width=\"400\" caption=\"RNN predicting after every token\" id=\"stateful_rep\" src=\"images/att_00024.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we had constructed tuples of dependent and independent variables such that there were pairs\n",
    "of three input words and one output word..So now we would have input and output sequences such that \n",
    "for each input word there will be output(its next word).Instead of 3,sequence length(sl) attribute is \n",
    "used >3.We repreat the process we had done earlier to create list of tuples containing pairs of \n",
    "dependent and independent variables and then we define the index for training and validation set and\n",
    "create dataloaders by passing the \"group_chunks\" function with the index for splitting.\"drop_last\"=\n",
    "True is passed so as to drop last batch as it is not of the same batch size and shuffle=False is \n",
    "passed to maintain the sequence of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing dataset for the next word prediction from each word\n",
    "sl = 16#sequence length\n",
    "seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))#list of tuples with input and output pairs.\n",
    "         for i in range(0,len(nums)-sl-1,sl))#iterating through whole word tokens\n",
    "cut = int(len(seqs) * 0.8)#index for training and validation split\n",
    "dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),#returns training split\n",
    "                             group_chunks(seqs[cut:], bs),#returns validation set\n",
    "                             bs=bs, drop_last=True, shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the first element in seqs.Contains two lists of equal length.One of them is the input and \n",
    "other is the output(contains next word for each word in first list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(#16) ['one','.','two','.','three','.','four','.','five','.'...],\n",
       " (#16) ['.','two','.','three','.','four','.','five','.','six'...]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the first element in the seqs \n",
    "[L(vocab[o] for o in s) for s in seqs[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the forward function in the model has to be modified such that it predicts a word after each word\n",
    "passed and not at the end of three word sequence.So we initialize a outs array which appends the \n",
    "predicted output word for every input word.It then returns a stacked array(with changed shape) \n",
    "containing the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel4(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)#input layer  \n",
    "        self.h_h = nn.Linear(n_hidden, n_hidden)#hidden layer \n",
    "        self.h_o = nn.Linear(n_hidden,vocab_sz)#output layer\n",
    "        self.h = 0 #initializing hidden state\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = []#initializing outs array for storing predictions\n",
    "        for i in range(sl):#iterating through each word in the sequence length\n",
    "            self.h = self.h + self.i_h(x[:,i])#passing word through input layer\n",
    "            self.h = F.relu(self.h_h(self.h))#getting activations through hidden layer\n",
    "            outs.append(self.h_o(self.h))#adding the predictions to outs array\n",
    "        self.h = self.h.detach()#detaching to erase the previous gradient memory\n",
    "        return torch.stack(outs, dim=1)#returning all predictions\n",
    "    \n",
    "    def reset(self): self.h = 0 #model resetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns output such that its shape is bs*sl*vocab-size.(stacking was done for dim=1).Targets\n",
    "are also of shape bs*sl*(batch size *sequence length).Since it returns probability of prediction for\n",
    "all the words in the vocab,so predictions are stacked  at dim=1.Likewise targets also need to be \n",
    "stacked before passsing them through the loss function.So we define a separate loss function \n",
    "\"loss_func\" which takes the predictions and targets and flattens both target and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function(predictions,targets)\n",
    "def loss_func(inp, targ):\n",
    "    return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))#returns the cross entropy from\n",
    "#flattened predictions and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next this loss function is passed through the learner along with the dataloaders,model architecture,\n",
    "loss function,metrics and the ModelResetter.This model should be trained for more number of epochs\n",
    "because the predictions have changed and therefore it requires more training time as its learning more\n",
    "information.Since the network is very deep,so everytime we train it,everytime different results are \n",
    "obtained as small or big gradients can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.285931</td>\n",
       "      <td>3.072032</td>\n",
       "      <td>0.212565</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.330371</td>\n",
       "      <td>1.969522</td>\n",
       "      <td>0.425781</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.742317</td>\n",
       "      <td>1.841378</td>\n",
       "      <td>0.441488</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.470120</td>\n",
       "      <td>1.810856</td>\n",
       "      <td>0.494303</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.298811</td>\n",
       "      <td>1.823132</td>\n",
       "      <td>0.492839</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.177598</td>\n",
       "      <td>1.769712</td>\n",
       "      <td>0.507568</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.074456</td>\n",
       "      <td>1.784031</td>\n",
       "      <td>0.492594</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.980687</td>\n",
       "      <td>1.782864</td>\n",
       "      <td>0.510661</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.902815</td>\n",
       "      <td>1.692381</td>\n",
       "      <td>0.567057</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.841107</td>\n",
       "      <td>1.680223</td>\n",
       "      <td>0.572754</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.787820</td>\n",
       "      <td>1.646383</td>\n",
       "      <td>0.599121</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.748949</td>\n",
       "      <td>1.684532</td>\n",
       "      <td>0.585612</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.716079</td>\n",
       "      <td>1.743749</td>\n",
       "      <td>0.592122</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.696397</td>\n",
       "      <td>1.751290</td>\n",
       "      <td>0.592367</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.682456</td>\n",
       "      <td>1.727617</td>\n",
       "      <td>0.598796</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating a learner\n",
    "learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,\n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 3e-3)#training model for more number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got better results from previous model as we train for more number of epochs and instead of \n",
    "predicting one word for a sequence of three words,we output one word for each word in the sequence.\n",
    "Next we can make our model better by getting more deeper model.\n",
    "\n",
    "The basic architecture of our model is small now as it contains one input layer,one hidden layer and \n",
    "one output layer.Every word in the sequence is passed through each of these layers and predictions are\n",
    "obtained for each word.If there are more layers the model can be improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multilayered RNN,instead of multiple layers there are multiple RNNs present.Activations are sent\n",
    "from one RNN to other RNN.\n",
    "<img alt=\"2-layer RNN\" width=\"550\" caption=\"2-layer RNN\" id=\"stacked_rnn_rep\" src=\"images/att_00025.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementing this in model,we need not create multiple layers in our code.Instead this can be done\n",
    "using Pytorch's RNN class part of nn.module which has the same architecture as the RNN we had created\n",
    "earlier and also allows us to stack multiple RNNs together.While creating the learner here,we need to\n",
    "pass the number of total RNNs in the network too along with the vocab size and the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this multilayered RNN model,we pass the information from one RNN to other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel5(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)#input layer\n",
    "        self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)#the second RNN\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)#output layer\n",
    "        self.h = torch.zeros(n_layers, bs, n_hidden)#initializing the h to zero matrix\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res,h = self.rnn(self.i_h(x), self.h)#passing the input activation and the hidden layer \n",
    "        #activation through second RNN\n",
    "        self.h = h.detach()#detaching h to remove gradient history\n",
    "        return self.h_o(res)#returning the output\n",
    "    \n",
    "    def reset(self): self.h.zero_()#model resetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model contains an extra RNN created through Pytorch's nn.RNN class which takes the activations of\n",
    "first RNN as input and its activations are passed through the output layer to get the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create learner and train the model in the same way with more number of epochs in the same way\n",
    "as earlier.Only difference here is that while instantiating model architecture,we also pass the number\n",
    "of total layered RNNs in the network through it apart from vocab size and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.041790</td>\n",
       "      <td>2.548714</td>\n",
       "      <td>0.455811</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.128514</td>\n",
       "      <td>1.708763</td>\n",
       "      <td>0.471029</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.699163</td>\n",
       "      <td>1.866050</td>\n",
       "      <td>0.340576</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.499681</td>\n",
       "      <td>1.738478</td>\n",
       "      <td>0.471517</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.339090</td>\n",
       "      <td>1.729538</td>\n",
       "      <td>0.494792</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.206317</td>\n",
       "      <td>1.835856</td>\n",
       "      <td>0.502848</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.088241</td>\n",
       "      <td>1.845554</td>\n",
       "      <td>0.520101</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.982788</td>\n",
       "      <td>1.856255</td>\n",
       "      <td>0.522624</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.890793</td>\n",
       "      <td>1.940331</td>\n",
       "      <td>0.525716</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.809587</td>\n",
       "      <td>2.028803</td>\n",
       "      <td>0.529785</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.743084</td>\n",
       "      <td>2.074600</td>\n",
       "      <td>0.535075</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.694128</td>\n",
       "      <td>2.153411</td>\n",
       "      <td>0.540039</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.660761</td>\n",
       "      <td>2.137608</td>\n",
       "      <td>0.547689</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.640680</td>\n",
       "      <td>2.169349</td>\n",
       "      <td>0.547363</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.630333</td>\n",
       "      <td>2.168200</td>\n",
       "      <td>0.548828</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating learner \n",
    "learn = Learner(dls, LMModel5(len(vocab), 64, 2), \n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 3e-3)#Training the model for more number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we mentioned that with deeper architecture models we would get better results.But this is not \n",
    "true here.The previous single layered RNN had better accuracy of 62% more than this model which gives\n",
    "around 54% of accuracy.Why multilayered RNNs cannot perform better is because of the problem of \n",
    "vanishing or exploding and activations.What do we mean by exploding or vanishing activations.Let's \n",
    "discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding or Disappearing Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we had constructed 3 RNN models with some differences each time introducing some change in \n",
    "the preceding model thus leading to improved results.In the previous model we tried to create a \n",
    "multilayered RNN thus creating a deeper architecture.But this model gives less accuracy than the model\n",
    "containing the single RNN model.In actual getting accurate results using such RNN model is very \n",
    "difficult.We can get good results in a model where detach is called less times and there are more\n",
    "layers.This leads to model learning more and important information.But more number of layers mean the \n",
    "model has a deeper architecture and deeper architectures have problems of exploding or vanishing \n",
    "activations.How can we overcome this and why at all this happens...\n",
    "\n",
    "In deeper architectures since there are many layers,there are many activations and parameters thus,\n",
    "the input matrix is multiplied several times.When we multiply by a number several times like in a \n",
    "Geometric Progression(GP),suppose first number is 1,then we multiply by 2 and if we keep doing so we\n",
    "get 2,4,8,16 and so on.We get a large number very soon in this series.But let's say we multiply by a \n",
    "small number say 0.5 we get 0.5,0.25,0.125 and so on...The number keeps decreasing exponentially and\n",
    "becomes close to zero.This is called exploding or vanishing activations.Continuous multiplication with\n",
    "numbers greater or smaller than 1 results in either a very big number thus exploding the initial no or\n",
    "very less number in this case vanishing the initial number after numbers are repeatedly multiplied.\n",
    "\n",
    "In activations,matrix multiplication takes place where numbers are multiplied and added and in deep\n",
    "neural networks there are many layers that's why continuous matrix multiplication results in very \n",
    "small or very large numbers at the end of neural networks.\n",
    "\n",
    "Computers store numbers as floating point numbers and hence they become less accurate as they go away\n",
    "from zero(positive or negative).This inaccuracy results in gradients becoming zero or infinity for \n",
    "very deep networks and hence weights are not updated properly.This ultimately leads to no improvement\n",
    "during the training as weights are not updated.\n",
    "\n",
    "There are several approaches to this problem such as batch normalization or modification in \n",
    "initialization which will be discussed in further lessons.here we will discuss about approaches used\n",
    "in RNN for avoiding exploding activations that is using \"__Gated Recurrent Units__\" (GRU) and \"__Long\n",
    "short-term memory__\"(LSTM) layers.Let'd discuss LSTMs first..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM was introduced back then in 1997.The difference here is that here there are two hidden states \n",
    "unlike one in the previous RNN models.The base RNN has the hidden state which is the output of the \n",
    "RNN at the previous time step.This hidden state carries information to predict the next correct token\n",
    "and memory of the previous words in the sentence.\n",
    "\n",
    "For example,Sentences \"Tom has a dog and he likes it very much\" and \"Siya has a dog and she likes it \n",
    "very much\".Now to predict he/she model should remember the first noun in the sentence then only it can\n",
    "predict if it is he/she or his/her.\n",
    "\n",
    "RNNs do not remember the previous information in the sentence,that is why there are 2 hidden states in\n",
    "LSTMs.The another hidden state is also called cell state.It ensures about the previous history of the\n",
    "sentence and the other hidden state focuses on predicting the next word in the sentence.Let's see how\n",
    "can we build a LSTM from scratch.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand LSTM architecture first...Below image describes LSTM architecture...\n",
    "<img src=\"images/LSTM.png\" id=\"lstm\" caption=\"Architecture of an LSTM\" alt=\"A graph showing the inner architecture of an LSTM\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Xt is the input here,it enters through the previous hidden state(h(t-1)) and cell state(c(t-1)).\n",
    "The orange boxes are the four layers where activations are sigmoid or tanh.tanh is also like sigmoid\n",
    "function only except that the output is ranged in (-1,1) unlile sigmoid where it lies between 0 and 1.\n",
    "\n",
    "<img src=\"https://machinelearningblogcom.files.wordpress.com/2017/11/bildschirmfoto-2017-11-10-um-12-20-57.png?w=428&h=237\"  caption=\"tanh activation function\" alt=\"A graph showing tanh activation function\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So apart from tanh the other activtion function is \"σ\",sigmoid.The green circles represent some \n",
    "elementwise operations.On the right side of the box,two signals are sent out.They are the new hidden \n",
    "state(h(t)) and the new cell state(c(t)) respectively which will be fed as next input.The new hidden \n",
    "state also acts as output that's why it splits while going out of box.\n",
    "\n",
    "The four neural nets are also called \"gates\".Also the cell state in previous time step c(t-1) changes\n",
    "very little when it goes out as c(t).From the figure also we can observe that it never passes directly\n",
    "through any neural net.That is why it is able to carry the information for long term.We will talk\n",
    "about all the four gates(neural nets) one by one.\n",
    "\n",
    "First gate also called \"__forget gate__\", input(xt) and the previous hidden state h(t-1) enter the box together.Previously also when we \n",
    "had written RNN architecture,the input embedding and the previous activation were added but in LSTM \n",
    "they are stacked together in big tensor.The dimension of xt can be different from h(t-1).The signal \n",
    "reaching all the neural nets is xt and h(t-1) together so its dimension is the sum of their sizes.\n",
    "If \"__n_in__\" and \"__n_hid__\" are the sizes of x(t) and h(t-1) respectively then all the neural nets\n",
    "have \"__n_in__+__n_hid__\" inputs and \"__n_hid__\" outputs.\n",
    "\n",
    "Since there is a sigmoid activation,the output is between 0 and 1.The result is multiplied by c(t-1)\n",
    "to see which information should go forward.The values which are nearly 0 are thrown and values near \n",
    "to 1 are kept.This helps in deciding which long short term memory is important and which is not.\n",
    "\n",
    "The second gate is called \"__input gate__\" and it also works with third gate(called cell gate) to update \n",
    "the cell state.Similar to the first gate,it also has a sigmoid activation and decides where cell state\n",
    "should be updated(close to 1) and where it should be not(nearly 0).The third gate has tanh activation\n",
    "and it scales the updated values in -1 to 1 range.The final result after the third gate activation is \n",
    "added to the cell state.\n",
    "\n",
    "The last gate is called the \"__output gate__\".It decides which information from cell state has to be \n",
    "included in the output.The cell state passes through tanh before it is integrated with the output from\n",
    "output gate.Finally the next time step hidden state is released(ht)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these four gates and the operations are implemented in code in a LSTM.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an LSTM from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous section,LSTM has two hidden states and four gates.The four gates are \n",
    "\"forget gate\",\"input gate\",\"cell gate\" and \"output gate\".All these are linear layers.Let's see a \n",
    "scratch implementation of LSTM.\"__init__\" contains all the linear layers and in the forward function\n",
    "we define the operations and the activations as explained earlier.Also \"__ni__\" and \"__nh__\" are the\n",
    "dimensions of input and hidden state respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        self.forget_gate = nn.Linear(ni + nh, nh)#First gate-the forget gate\n",
    "        self.input_gate  = nn.Linear(ni + nh, nh)#Second gate-the input gate\n",
    "        self.cell_gate   = nn.Linear(ni + nh, nh)#third gate-the cell gate\n",
    "        self.output_gate = nn.Linear(ni + nh, nh)#fourth gate-the output gate\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        h,c = state#the current cell and hidden state\n",
    "        h = torch.stack([h, input], dim=1)#stacked/combined with input\n",
    "        forget = torch.sigmoid(self.forget_gate(h))#sigmoid activation through forget_gate\n",
    "        c = c * forget#multiplied with cell state\n",
    "        inp = torch.sigmoid(self.input_gate(h))#sigmoid activation of hidden state through input gate\n",
    "        cell = torch.tanh(self.cell_gate(h))#tanh activation of hidden state through cell gate.\n",
    "        c = c + inp * cell #celll gate and input gate output multiplied and added to cell state\n",
    "        out = torch.sigmoid(self.output_gate(h))#sigmoid activation of output gate\n",
    "        h = outgate * torch.tanh(c)#part of cell state(to be integrated in output) passes through tanh\n",
    "        return h, (h,c) #returns the next hidden state(to be integrated in next output) and the next\n",
    "        #hidden and cell state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above code can be written using refactoring.Above we are doing matrix multiplication 4 times \n",
    "which can be reduced to one big matrix multiplication.It optimizes performance,saves time and memory \n",
    "too.Instead of four layers,two separate layers are used for input state and hidden state.Let's see how\n",
    "code looks like after refactoring.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        self.ih = nn.Linear(ni,4*nh)#Layer for input state\n",
    "        self.hh = nn.Linear(nh,4*nh)#layer for hidden state\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        h,c = state\n",
    "        # One big multiplication for all the gates is better than 4 smaller ones\n",
    "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)\n",
    "        ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])#sigmoid activation at three gates\n",
    "        cellgate = gates[3].tanh()#tanh activation at third gate\n",
    "        c = (forgetgate*c) + (ingate*cellgate)#updating cell state\n",
    "        h = outgate * c.tanh() #passing the cellstate through tanh and then multiplying by outgate for\n",
    "        #final hidden state\n",
    "        return h, (h,c) #return the next hidden state(integrated with output) and hidden state and \n",
    "        #cell state for next time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we refactor the LSTM architecture instead of four smll matrix multiplication,it is replaced with \n",
    "one big matrix multiplication.Signoid activation is also applied to three gates all at once.Instead\n",
    "of four different layers for four gates,two layers for an input state and hidden state are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch's chunk method is used to split the tensor into 4 parts.We need to pass the number of parts we\n",
    "want after the divison.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(0,10); t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pytorch's chunk divides tensor into 2 parts...\n",
    "t.chunk(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture can be used to train a Language Model now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Language Model Using LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we had trained multilayered RNN using a two layered RNN architecture but it did not give \n",
    "better accuracy.Here we are training the same model but using a two layer.Pytorch's nn.LSTM class\n",
    "provides us the opportunity so that we can access the LSTM architecture without writing the code for \n",
    "the same.We train the model at higher learning rate and for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel6(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)#input layer\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)#the LSTM architecture\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)#output layer\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]#initializing hidden state\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res,h = self.rnn(self.i_h(x), self.h)#passing the input and hidden state through LSTM\n",
    "        self.h = [h_.detach() for h_ in h]#erasing the gradient history\n",
    "        return self.h_o(res)#returning the cell state after passing through the output layer\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()#model resetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.026114</td>\n",
       "      <td>2.772102</td>\n",
       "      <td>0.153076</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.216185</td>\n",
       "      <td>2.089065</td>\n",
       "      <td>0.269124</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.613936</td>\n",
       "      <td>1.826282</td>\n",
       "      <td>0.478760</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.316553</td>\n",
       "      <td>2.065686</td>\n",
       "      <td>0.505290</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.092289</td>\n",
       "      <td>2.028179</td>\n",
       "      <td>0.590088</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.857714</td>\n",
       "      <td>1.954994</td>\n",
       "      <td>0.656087</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.604967</td>\n",
       "      <td>1.934748</td>\n",
       "      <td>0.682292</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.396310</td>\n",
       "      <td>1.708010</td>\n",
       "      <td>0.699870</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.244986</td>\n",
       "      <td>1.690051</td>\n",
       "      <td>0.734294</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.150833</td>\n",
       "      <td>1.777465</td>\n",
       "      <td>0.741048</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.098923</td>\n",
       "      <td>1.731341</td>\n",
       "      <td>0.752034</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.070643</td>\n",
       "      <td>1.755179</td>\n",
       "      <td>0.756592</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.054341</td>\n",
       "      <td>1.766199</td>\n",
       "      <td>0.755127</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.043541</td>\n",
       "      <td>1.766006</td>\n",
       "      <td>0.757406</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.038261</td>\n",
       "      <td>1.774114</td>\n",
       "      <td>0.757894</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating a learner and passing the above model architecture through it.\n",
    "learn = Learner(dls, LMModel6(len(vocab), 64, 2), #Pass vocabsize,batch size,no of layers through the \n",
    "                #Model architecture...\n",
    "                loss_func=CrossEntropyLossFlat(), \n",
    "                metrics=accuracy, cbs=ModelResetter)\n",
    "learn.fit_one_cycle(15, 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy is better than the one obtained through multilayered RNN.The training loss is decreasing\n",
    "but validation loss is not so good,this shows that the model is overfitting a little.We can apply \n",
    "regularization here like we had applied it in Machine Learning.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had already discussed about the problems in training RNN because of the vainishing activations and \n",
    "gradients problem.We also discussed in the previous section that how we can use LSTMs to overcome this\n",
    "issue.But LSTM models tend to overfit leading to high validation loss.We had discussed about data \n",
    "augmentation previously for Image data to avoid overfitting.But it is mostly used for images rather\n",
    "than text data as different variants can be created.Data Augmentation for text requires different \n",
    "models to convert text into some other language which is a tiresome task.Data augmentation is not \n",
    "well explored for text data yet.\n",
    "\n",
    "Some other regularization techniques are also being used to reduce overfitting for LSTM models.Let's \n",
    "study how we can prevent overfitting using methods such as dropout,activation regularization and \n",
    "temporal activation regularization.The researchers have also called such method \"__AWD-LSTM__\".Let's\n",
    "discuss each one by one..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout was introduced by Geoferry Hinton,the father of Deep learning himself.The method involves \n",
    "making some activations zero at training time.This ensures that all neurons work actively.\n",
    "\n",
    "<img src=\"https://miro.medium.com/proxy/1*iWQzxhVlvadk6VAJjsgXgg.png\" alt=\"A figure from the article showing how neurons go off with dropout\" width=\"800\" id=\"img_dropout\" caption=\"Applying dropout in a neural network (courtesy of Nitish Srivastava et al.)\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above figure shows how some neurons are inactivated.Dropout makes activations noisy and generalize\n",
    "model better.But the activations cannot just be removed like that.Thus dropout is applied with a \n",
    "probability that the activations are removed with some probability.Suppose there are 5 activations,\n",
    "then if dropout is applied with a probability of p,all the activations are rescaled by dividing by 1-p\n",
    ".Full implementation of dropout in Pytorch is as follows:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p): self.p = p #probability for inactivation\n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        mask = x.new(*x.shape).bernoulli_(1-p)\n",
    "        return x * mask.div_(1-p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"bernoulli_\" method creates a tensor of random zeros(probability p) and ones(probability 1-p).It is \n",
    "multiplied by input before dividing by 1-p.Drop is used before passing the output of the lSTM to final\n",
    "output layer.It can also be used in other models including CNN and also used as other name \"ps\" attribute\n",
    "in tabular module of fastai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does different processes while training and validation.That's why in Dropout class above training \n",
    "condition was checked once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Regularization and Temporal Activation Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn about activation regularization and temporal activation regularization.From the names \n",
    "again it suggests that we would be making some modifications in activations only.Previously we had \n",
    "discussed about weight decay also.In weight decay,penalty is added to the loss so as to reduce the \n",
    "value of weights.In activation regularization instead of weights,activations are minimized.\n",
    "The mean of the squares of the activations is added to the the loss and multiplied by a Loss \n",
    "coefficient alpha.\n",
    "\n",
    "``` python\n",
    "loss += alpha * activations.pow(2).mean()\n",
    "```\n",
    "\n",
    "Temporal activation regularization(TAR) is related to the networks and memory of the model.Since in \n",
    "language model we are predicting the next words in the sentence.TAR improves this property of the \n",
    "model by adding a penalty to the loss and tries to minimize the difference between two consecutive\n",
    "activations.The penalty component takes the mean of the squares of difference of the consecutive\n",
    "ativations.So loss can be written as :-\n",
    "    \n",
    "\n",
    "``` python\n",
    "loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha and beta are hyperparameters needed to be tuned like the ones in L1 and L2 regularization.\n",
    "To apply regularization dropout should be applied on the model then model should output three \n",
    "things,the normal output,the activations before applying dropout and the activations after \n",
    "applying dropout.AR and TAR both are applied to predropout and postdropout activations \n",
    "respectively.We do not have to write long codes for doing this.While we create a learner,a \n",
    "callback called \"RNNRegularizer \" , is passed with alpha and beta values which applies regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Weight-Tied Regularized LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine dropout with AR and TAR to apply regularization.Previous section we discussed that we need \n",
    "three outputs from our model the normal output,the dropped out activations and the LSTM activations.\n",
    "Then \"__RNNRegularizer__\" does TAR and AR and adds the penalty to the loss component.\n",
    "\n",
    "One more thing which can be applied is weight tying.In Language model the input embeddings are mapped\n",
    "from words to activations.And the output layers is mapped from activations to words.These mappings\n",
    "could be same and we can do this by applying the same weight matrix to the embedding and the hidden \n",
    "layer.It can be done at the end by just one line of code and Pytorch allows us to do that..\n",
    "\n",
    "self.h_o.weight=self.i_h.weight\n",
    "\n",
    "Let's write the final architecture for a regularized model where we apply dropout,AR,TAR and also \n",
    "weight tying.Dropout is applied by Pytorch's nn.Dropout(p) class where probability is passed.AR and \n",
    "TAR is applied by passing RNNRegularizer callback in Learner.Weight tying is applied by making\n",
    "weight matrix of the embedding and the output layer same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel7(Module):\n",
    "    def __init__(self, vocab_sz, n_hidden, n_layers, p):\n",
    "        self.i_h = nn.Embedding(vocab_sz, n_hidden)#input Embedding layer\n",
    "        self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)#LSTM cell\n",
    "        self.drop = nn.Dropout(p)#Dropout applied\n",
    "        self.h_o = nn.Linear(n_hidden, vocab_sz)#output layer\n",
    "        self.h_o.weight = self.i_h.weight#Weight matrix made equal,weight tying applied\n",
    "        self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)]#Hidden state initialized to \n",
    "        #zero\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raw,h = self.rnn(self.i_h(x), self.h)#passing the input and the hidden state through the LSTM\n",
    "        #cell\n",
    "        out = self.drop(raw)#Dropout applied to the LSTM activation\n",
    "        self.h = [h_.detach() for h_ in h]#Erasing gradient history\n",
    "        return self.h_o(out),raw,out#returning the output,the dropped out activations and the LSTM \n",
    "        #activation(for Regularization)\n",
    "    \n",
    "    def reset(self): \n",
    "        for h in self.h: h.zero_()#Model resetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a learner is created with the LMModel7 architecture.The vocabsize,batch size,number of layers\n",
    "and probability is passed through the architecture and rest we pass loss function,metrics and the two\n",
    "callback for Model resetting andd regularization with the alpha and beta values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the learner\n",
    "learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),\n",
    "                #model architecture(vocab size,batch size,number of layers,probability)\n",
    "                loss_func=CrossEntropyLossFlat(), metrics=accuracy,\n",
    "                cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using fastai we can create TextLearner which by default passes the callbacks for regularization and \n",
    "reset through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the text learner for the model\n",
    "learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),\n",
    "                    loss_func=CrossEntropyLossFlat(), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the model and we add an additional regularization by adding a wd(weight decay) to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.486498</td>\n",
       "      <td>1.885105</td>\n",
       "      <td>0.515055</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.596433</td>\n",
       "      <td>1.247586</td>\n",
       "      <td>0.631266</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.911280</td>\n",
       "      <td>0.764302</td>\n",
       "      <td>0.760579</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.532967</td>\n",
       "      <td>0.671182</td>\n",
       "      <td>0.797607</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.356029</td>\n",
       "      <td>0.573203</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.250572</td>\n",
       "      <td>0.594855</td>\n",
       "      <td>0.822673</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.196667</td>\n",
       "      <td>0.523730</td>\n",
       "      <td>0.837891</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.167411</td>\n",
       "      <td>0.555765</td>\n",
       "      <td>0.824626</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.147350</td>\n",
       "      <td>0.439533</td>\n",
       "      <td>0.872396</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.133110</td>\n",
       "      <td>0.430603</td>\n",
       "      <td>0.876709</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.122869</td>\n",
       "      <td>0.413723</td>\n",
       "      <td>0.873372</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.113981</td>\n",
       "      <td>0.407887</td>\n",
       "      <td>0.873535</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.107861</td>\n",
       "      <td>0.392057</td>\n",
       "      <td>0.879395</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.103829</td>\n",
       "      <td>0.400105</td>\n",
       "      <td>0.875244</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.101690</td>\n",
       "      <td>0.409799</td>\n",
       "      <td>0.872803</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training the model\n",
    "learn.fit_one_cycle(15, 1e-2, wd=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this model is not overfitted as it has a good training as well as validation loss.Also we have \n",
    "improved the accuracy for the model by 87% as compared to the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we learnt about training RNN and LSTM architecture from scratch.We also learnt about applying \n",
    "regularization to the our models about \"Dropout\",\"Activation Regularization\" and \"Temporal Activation\n",
    "Regularization\".In chapter-1 we used AWD-LSTM architecture in text classification.It applies dropout \n",
    "by default to embedding layer,input layer,weights of the LSTM layer and to the hidden state.Dropouts\n",
    "on so many activations makes the model more regularized.\n",
    "One more architecture which is useful for sequence models is the Transformers architecture.They are \n",
    "also mainly used for sequence to sequence problems such as language translation.We can learn more \n",
    "about them in later chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If the dataset for your project is so big and complicated that working with it takes a significant amount of time, what should you do?\n",
    "1. Why do we concatenate the documents in our dataset before creating a language model?\n",
    "1. To use a standard fully connected network to predict the fourth word given the previous three words, what two tweaks do we need to make to ou model?\n",
    "1. How can we share a weight matrix across multiple layers in PyTorch?\n",
    "1. Write a module that predicts the third word given the previous two words of a sentence, without peeking.\n",
    "1. What is a recurrent neural network?\n",
    "1. What is \"hidden state\"?\n",
    "1. What is the equivalent of hidden state in ` LMModel1`?\n",
    "1. To maintain the state in an RNN, why is it important to pass the text to the model in order?\n",
    "1. What is an \"unrolled\" representation of an RNN?\n",
    "1. Why can maintaining the hidden state in an RNN lead to memory and performance problems? How do we fix this problem?\n",
    "1. What is \"BPTT\"?\n",
    "1. Write code to print out the first few batches of the validation set, including converting the token IDs back into English strings, as we showed for batches of IMDb data in <<chapter_nlp>>.\n",
    "1. What does the `ModelResetter` callback do? Why do we need it?\n",
    "1. What are the downsides of predicting just one output word for each three input words?\n",
    "1. Why do we need a custom loss function for `LMModel4`?\n",
    "1. Why is the training of `LMModel4` unstable?\n",
    "1. In the unrolled representation, we can see that a recurrent neural network actually has many layers. So why do we need to stack RNNs to get better results?\n",
    "1. Draw a representation of a stacked (multilayer) RNN.\n",
    "1. Why should we get better results in an RNN if we call `detach` less often? Why might this not happen in practice with a simple RNN?\n",
    "1. Why can a deep network result in very large or very small activations? Why does this matter?\n",
    "1. In a computer's floating-point representation of numbers, which numbers are the most precise?\n",
    "1. Why do vanishing gradients prevent training?\n",
    "1. Why does it help to have two hidden states in the LSTM architecture? What is the purpose of each one?\n",
    "1. What are these two states called in an LSTM?\n",
    "1. What is tanh, and how is it related to sigmoid?\n",
    "1. What is the purpose of this code in `LSTMCell`: `h = torch.stack([h, input], dim=1)`\n",
    "1. What does `chunk` do in PyTorch?\n",
    "1. Study the refactored version of `LSTMCell` carefully to ensure you understand how and why it does the same thing as the non-refactored version.\n",
    "1. Why can we use a higher learning rate for `LMModel6`?\n",
    "1. What are the three regularization techniques used in an AWD-LSTM model?\n",
    "1. What is \"dropout\"?\n",
    "1. Why do we scale the weights with dropout? Is this applied during training, inference, or both?\n",
    "1. What is the purpose of this line from `Dropout`: `if not self.training: return x`\n",
    "1. Experiment with `bernoulli_` to understand how it works.\n",
    "1. How do you set your model in training mode in PyTorch? In evaluation mode?\n",
    "1. Write the equation for activation regularization (in math or code, as you prefer). How is it different from weight decay?\n",
    "1. Write the equation for temporal activation regularization (in math or code, as you prefer). Why wouldn't we use this for computer vision problems?\n",
    "1. What is \"weight tying\" in a language model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans-1 It is always good to start with simple and small datasets.If it takes a long time training the\n",
    "dataset,the learning rate can be increased.\n",
    "\n",
    "Ans-2 In a language model since we are predicting a sequence to sequence problem so its better to \n",
    "concatenate the text so that when model is trained,it learns the information about previous parts in \n",
    "the sentence too.\n",
    "\n",
    "Ans-3 To predict next word in model using the previous three words as input,we use a standard three \n",
    "layer model.The tweaks are that all the three layers use same weight matrix and that first layer uses\n",
    "first word's embedding as input and second layer uses second word's embeddings and first layer's \n",
    "output activations as input and so on for the third layer too.\n",
    "\n",
    "Ans-4 Since the weights are same so repeat layer concept is used.We create one layer and we can use it\n",
    "later multiple times.\n",
    "\n",
    "Ans-5 Same as the module for the three word model except that the Dataset and sequence creation will \n",
    "be different.Independent variable would have 2 words instead of 3 words.\n",
    "\n",
    "Ans-6 A recurrent neural network can take variable length inputs and produce variable length outputs.\n",
    "It has a hidden state(h) which is updated at every time step and uses same weights for every layer,\n",
    "that's why it is called recurrent.That is why they are used for processing sequences like text data.\n",
    "\n",
    "Ans-7 Recurrent Neural networks loop the information to flow from one step to next.The information is\n",
    "stored in hidden state which is like contribution from the previous steps input and fed to next steps\n",
    "as input.It is updated every time.\n",
    "\n",
    "Ans-8 There is a middle hidden layer(h_h) which acts as the hidden state in LMModel1.\n",
    "\n",
    "Ans-9 If the hidden state is initialized 0 for every new input,model doesn't learn anything about\n",
    "previou information in the sentence,therefore text is passed in order to the model so that model \n",
    "learns and remembers the previous information and hidden state is initialized in the \"__init__\" \n",
    "function and this makes the model very deep with aroune 10000 layers or more if there are 10000 tokens\n",
    "in the text.\n",
    "\n",
    "Ans-10 Whenever a RNN is created without the for loop where each layer corresponds to each word as \n",
    "input and there is no looping.That is called \"unrolled representation\" of a RNN.It is after that we\n",
    "use for loop for refactoring the network and we make the network fast using the detach method in the \n",
    "forward,the model remembers the activations between different layers.\n",
    "\n",
    "Ans-11 We maintain the hidden state by initializing the hidden state in \"__init__\".But this makes the \n",
    "model very deep like some 10000 layered model so when gradient is calculated for the last layer,the \n",
    "gradients have to be calculated right from the first layer.This becomes very slow and memory consuming\n",
    ".Thus,instead of propogating throughout the network till first layer,only three layers are kept and \n",
    "and the whole network history is erased using detach method.\n",
    "\n",
    "Ans-12 After refactoring the model,the architecture becomes more deep and backpropogation to calculate\n",
    "gradient for 10000 layers would be very slow and memory consuming so gradients are calculated only for\n",
    "the last three tokens instead of the whole stream.That is called backpropogation through time(bptt).It\n",
    "basically truncates the memory of the gradient history after a few time steps.\n",
    "\n",
    "Ans-13 dls.valid.show_batch(),toks.decode(num_toks)\n",
    "\n",
    "Ans-14 ModelResetter callback actually calls the reset method of the model before every epoch and\n",
    "validation phase,it ensures a clean hidden phase before the start of each epoch so that it can read\n",
    "continuous chunks of text in the next epoch cycle.\n",
    "\n",
    "Ans-15 Predicting only one output word for three input words causes the model with less information or\n",
    "signal flow.Like we are sending the information of one word to update the weights of three words which\n",
    "is not very efficient.Instead if we could update weight for every word using one word only then it \n",
    "would be more efficient and more information could flow through it.\n",
    "\n",
    "Ans-16 In LMModel4 the shape of the inputs and outputs is changed a little.To create more signal in \n",
    "the model we predict one word for each word input so datasets is changed in such a way that for every\n",
    "three next words,there are three 3 input words.So a custom \"__sl__\".sequence length attribute is \n",
    "introduced.Thus model returns output of shape batch size * sequence length * vocab size (bs*sl*vocab\n",
    "size) and targets are of different dimensions so targets have to be flattened before applying the \n",
    "loss function.Thus a custom loss function is defined which changes the dimension of the targets and \n",
    "predictions accordingly.\n",
    "\n",
    "Ans-17 In LMModel 4 we use multilayered RNNs to stack RNNs and train the model.This causes model to\n",
    "become more deep and continuous additions and multiplications results in vanishing gradients or \n",
    "exploding gradients problem.Since computer stores numbers as floating point numbers,so inaccuracy \n",
    "increases as the numbers go away from zero and the gradients become either zero or infinite which \n",
    "results in weights not getting updated or becoming zero.\n",
    "\n",
    "Ans-18 In an unrolled representation,if we use one letter to predict each next word,the model becomes\n",
    "more complicated and with more layers it takes a longer time to train.With stacking RNNs,we can train\n",
    "the same model in less time.\n",
    "\n",
    "Ans-20 \"__detach__\" is used for clearing the gradient history.Using detach less often would make our\n",
    "model more deeper and thus taking more time for training and extraction of more informative features.\n",
    "With simple RNNs,problem of vanishing or exploding gradients may occur.\n",
    "\n",
    "Ans-21 A deep network involves continuous matrix multiplications on the same matrix a number of times.\n",
    "This results in either numbers decreasing to zero or jumping to infinite.Thus activations are also \n",
    "exploded or they are vanished.Gradients used for updating weights end up becoming zero or infinite.\n",
    "With optimization the weights won't be improved during the training process.\n",
    "\n",
    "Ans-22 Since computers store numbers as floating point numbers,so they become less accurate as they \n",
    "move away from zero.\n",
    "\n",
    "Ans-23 Gradients are used to update weights during the training process.Vanishing gradients may lead\n",
    "to no changes in the weights and if weights are not updated then the model won't improve with training\n",
    ".The accuracy would remain same.\n",
    "\n",
    "Ans-24 LSTM architecture has two hidden states instead of one in RNNs.So here one hidden state also \n",
    "called cell state sometimes is responsible for maintaining the carry forward of the long short term\n",
    "memory and that's why it doesn't undergo much activations also whereas other hidden state is \n",
    "responsible for predicting the next token which undergoes through many layers and activations.\n",
    "\n",
    "Ans-25 Cell state and hidden state\n",
    "\n",
    "Ans-26 tanh is also an activations function like sigmoid.Only difference is that it rescales the \n",
    "values between -1 and 1 instead of 0 and 1.It is linearly related with sigmoid such that\n",
    "tanh(x)=2*σ(2x)-1.\n",
    "\n",
    "Ans-27 In the pictorial representation of the LSTM architecture it is very clear that the input and \n",
    "the hidden state at previous time step are joined together in the network.That's why after getting the\n",
    "hidden state and cell state in LSTM cell hidden state is updated.\n",
    "h = torch.stack([h, input], dim=1) joins the hidden state and the input matrix together in one\n",
    "dimension.\n",
    "\n",
    "Ans-28 Pytorch chunk divides a torch tensor into number of parts as passed as argument through the \n",
    "chunk method.They may have equal or unequal elements.\n",
    "\n",
    "Ans-29 The refactored version of the LSTM architecture works same but it saves memory for GPU\n",
    "processing.Instead of four matrix multiplications,one big matrix multiplications is done.The stacking \n",
    "is time consuming since GPU does many operations in parellel.So instead of making model more deep,two \n",
    "separate layers are used for input and hidden state.Further all the stacking and operations are \n",
    "performed in the forward function.\n",
    "\n",
    "Ans-30 In LMModel6,two layered LSTM network is used along with the input Embedding and the output \n",
    "layer.This model is trained for small number of epochs with a higher learning rate to attain better\n",
    "accuracy with small epochs.Higher learning rate though results in model being overfit sometimes.But \n",
    "that can be resolved by applying regularization procedures.\n",
    "\n",
    "Ans-31 Activation Regularization(A),Weight Tying(W) and Dropout(D) are regularization techniques in \n",
    "AWD-LSTM.\n",
    "\n",
    "Ans-32 Dropout is a regularization technique used for neural networks.It involves randomly making\n",
    "some activations zero at training time.It helps in model generalizing better by making activations \n",
    "more noisy and neurons interacting better with each other.\n",
    "\n",
    "Ans-33 During dropout if activations are just made zero without doing anything else,there would be \n",
    "problems in training the problem,like if from 5 activations,3 are dropped and we are left with 2,then \n",
    "it may cause problem in training.Therefore dropout is applied with a probability by rescaling them by\n",
    "dividing by 1-p.This results in p activations being zeroed and 1-p remaining.\n",
    "\n",
    "Ans-34 Dropout behaves differently during training and validation.In training time,the activation unit\n",
    "is present with probability p and in validation time,the activation unit is always present and weights\n",
    "are multiplied by p.\"__if not self.training: return x__\" checks if there is training phase or\n",
    "validation phase and accordingly returns the resulting activation.\n",
    "\n",
    "Ans-35 During Dropout we select activations with probability such that p activations are zeroed\n",
    "and 1-p are nonzero.So \"__bernoulli___\" method creates a tensor of random zeros(probability p) and \n",
    "random ones(probability(1-p)) which is multiplied by input before dividing by 1-p.\"Training\" or \n",
    "validation phases are checked using the training attribute in Pytorch's nn Module.\n",
    "\n",
    "Ans-36 We use train method in Module which sets training to True and evaluation set to False.It is \n",
    "done automatically while creating Learner.\n",
    "\n",
    "Ans-37 loss+=alpha*activations**2.mean().In weight decay we are trying to minimize the value of \n",
    "weights while in activation regularization,we control the value of activations.\n",
    "\n",
    "Ans-38 loss+=beta*(activations[:,1:]-activations[:,:-1])**2.mean().Temporal activation regularization\n",
    "tries to minimize the difference between two consecutive activations.In Computer vision problems\n",
    "activations from different layers are responsible for different features of the Data,so differencing\n",
    "of the activations may result in loss of important information and also noisy activations more.\n",
    "\n",
    "Ans-39 Weight tying in Language model means making weight matrix equal for the output layer and input\n",
    "Embedding layer.Since the input and output mappings are same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In ` LMModel2`, why can `forward` start with `h=0`? Why don't we need to say `h=torch.zeros(...)`?\n",
    "1. Write the code for an LSTM from scratch (you may refer to <<lstm>>).\n",
    "1. Search the internet for the GRU architecture and implement it from scratch, and try training a model. See if you can get results similar to those we saw in this chapter. Compare you results to the results of PyTorch's built in `GRU` module.\n",
    "1. Take a look at the source code for AWD-LSTM in fastai, and try to map each of the lines of code to the concepts shown in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
